{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_7PJiEjE1oQ"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5priM6Pj-Fct"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41JteDjjE6DP"
   },
   "source": [
    "# Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkXtbXsYE83A"
   },
   "source": [
    "## Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "70eoJYj69zL1"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalization_n1_p1(x):\n",
    "    return (x/127.5)-1\n",
    "\n",
    "def normalization_n0_p1(x):\n",
    "    return x/255\n",
    "\n",
    "def one_hot(y, num_classes=1):\n",
    "    #print(y.shape, y)\n",
    "    tmp = np.zeros((*y.shape, num_classes), dtype=np.float32)\n",
    "    tmp[:, y] = 1.0\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUAnrlLTE_bX"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZxRIJnYp_XH4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_dataset_fn(img, label):\n",
    "    info_str = \"\"\"img: shape {}, min {}, max {}, type {};\n",
    "label: shape {}, min {}, max {}, type {};\"\"\".format(img.shape, np.min(img), np.max(img), type(img),\n",
    "                                           label.shape, np.min(label), np.max(label), type(label))\n",
    "    print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69vODF9qFC1K"
   },
   "source": [
    "## Aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CVVWhWp293gP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ExtendedMNISTDataset(object):\n",
    "    def __init__(self, root: str = \"/kaggle/input/fii-atnn-2025-competition-1\", train: bool = True):\n",
    "        \"\"\"ExtendedMNISTDataset:\n",
    "        root  - path root of dataset\n",
    "        train - read train or test dataset (true - train, false - test)\n",
    "        preprocessing - image preprocesing function\n",
    "        Dataset structure:\n",
    "            list -> (batchsize, tuple -> (np.array -> (image->(784, )), scalar -> (labels->1)))\n",
    "        \"\"\"\n",
    "        # select filename\n",
    "        if (train):\n",
    "            file = \"extended_mnist_train.pkl\"\n",
    "        else:\n",
    "            file = \"extended_mnist_test.pkl\"\n",
    "        # join root to filename\n",
    "        filename = os.path.join(root, file)\n",
    "        # read dataset\n",
    "        dataset = self.__read(filename)\n",
    "        self.inputs, self.outputs = self.__split_data(dataset)\n",
    "\n",
    "    def __read(self, filename):\n",
    "        # try to open filename\n",
    "        try:\n",
    "            f = open(filename, \"rb\")\n",
    "            try:\n",
    "                dataset = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                dataset = None\n",
    "                self.__show_exception(e)\n",
    "            finally:\n",
    "                f.close()\n",
    "        except IOError as e:\n",
    "            dataset = None\n",
    "            self.__show_exception(e)\n",
    "        return dataset\n",
    "\n",
    "    def __show_exception(self, e) -> None:\n",
    "        tb = traceback.extract_tb(e.__traceback__)\n",
    "        last_call = tb[-1]\n",
    "        print(f\"❌ Error in function '{last_call.name}' at line {last_call.lineno}\")\n",
    "        print(f\"   File: {last_call.filename}\")\n",
    "        print(f\"   Exception: {e}\")\n",
    "\n",
    "    def __split_data(self, dataset):\n",
    "        inputs  = []\n",
    "        outputs = []\n",
    "        for input, ouput in dataset:\n",
    "            inputs.append(input)\n",
    "            outputs.append(ouput)\n",
    "        return np.array(inputs, dtype=np.uint8), np.array(outputs, dtype=np.int32)\n",
    "\n",
    "    def __len__(self, ) -> int:\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, i : int):# int|np array\n",
    "        return self.inputs[i], self.outputs[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFM2SE0RFK4R"
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NTBc5fYl-Cxk"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batchsize=1, shuffle=False):\n",
    "        assert (batchsize > 0), \"batchsize should be ghreat than 'zero'\"\n",
    "        assert (isinstance(shuffle, bool)), \"shuffle should be 'bool'\"\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "        self.size = len(self.dataset)\n",
    "        # shufle\n",
    "        if (self.shuffle):\n",
    "            self.permutation = np.random.permutation(self.size)\n",
    "        else:\n",
    "            self.permutation = np.arange(self.size, dtype=np.int32)\n",
    "        self.maps_fn = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)//self.batchsize\n",
    "\n",
    "    def __call__(self):\n",
    "        try:\n",
    "            for i in range(0, self.size, self.batchsize):\n",
    "                pos  = self.permutation[i:i+self.batchsize]\n",
    "                datas = self.dataset[pos]\n",
    "                datas = self.__map_fn(datas)\n",
    "                yield datas\n",
    "            else:\n",
    "                if (self.shuffle):\n",
    "                    self.permutation = np.random.permutation(self.size)\n",
    "        except Exception as e:\n",
    "            self.__show_exception(e)\n",
    "\n",
    "    def take(self, size):\n",
    "        try:\n",
    "            for i, data in zip(range(size), self()):\n",
    "                yield data\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            self.__show_exception(e)\n",
    "\n",
    "    def __show_exception(self, e) -> None:\n",
    "        tb = traceback.extract_tb(e.__traceback__)\n",
    "        last_call = tb[-1]\n",
    "        print(f\"❌ Error in function '{last_call.name}' at line {last_call.lineno}\")\n",
    "        print(f\"   File: {last_call.filename}\")\n",
    "        print(f\"   Exception: {e}\")\n",
    "\n",
    "    def map(self, fn):\n",
    "        self.maps_fn.append(fn)\n",
    "\n",
    "    def __map_fn(self, data):\n",
    "        for fn in self.maps_fn:\n",
    "            data = fn(*data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdB1TO4bFROP"
   },
   "source": [
    "## Test Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TXJT28zC-gJQ"
   },
   "outputs": [],
   "source": [
    "# Pipiline\n",
    "root = \"/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/fii-atnn-2025-competition-1\"\n",
    "train_ds = ExtendedMNISTDataset(root=root, train=True)\n",
    "test_ds = ExtendedMNISTDataset(root=root, train=False)\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_ds, batchsize=2, shuffle=True)\n",
    "train_loader.map(lambda x, y: (normalization_n1_p1(x), one_hot(y, num_classes=10)))\n",
    "train_loader.map(lambda x, y: (np.expand_dims(x, axis=-1), np.expand_dims(y, axis=-1)))\n",
    "#test_loader  = DataLoader(test_ds,  batchsize=20, shuffle=False)\n",
    "#test_loader.map(lambda x, y: (normalization_n1_p1(x), one_hot(y, num_classes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_7OV7m3F-8HK",
    "outputId": "b56c90fc-6238-4f61-f0a2-1b2fc61159fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n",
      "img: shape (2, 784, 1), min -1.0, max 0.9921568627450981, type <class 'numpy.ndarray'>;\n",
      "label: shape (2, 10, 1), min 0.0, max 1.0, type <class 'numpy.ndarray'>;\n"
     ]
    }
   ],
   "source": [
    "for img, label in train_loader.take(10):\n",
    "    test_dataset_fn(img, label)\n",
    "    #print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mK8ixK8NFT1H"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ET83CeOrRu4P"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Acuracy():\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, y_pred, y):\n",
    "        # y_pred: logits or probabilities (batch_size, num_classes)\n",
    "        # y_true: true labels (batch_size,)\n",
    "        preds = np.argmax(y_pred, axis=1)\n",
    "        y     = np.argmax(y,      axis=1)\n",
    "        correct = (preds == y).sum()\n",
    "        total = y.shape[0]\n",
    "        return correct / total\n",
    "\n",
    "accuracy = Acuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrv0gwHHFcU6"
   },
   "source": [
    "#  Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvKEWsFDEzMS"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            w = np.expand_dims(w, axis=0)\n",
    "            a = np.matmul(w, a)+b\n",
    "            a = sigmoid(a)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, train_ds, epochs, eta=0.001):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        for j in range(epochs):\n",
    "            for i, data in zip(range(len(train_ds)), train_ds()):\n",
    "                self.update_mini_batch(data, eta)\n",
    "            acc = self.evaluate(train_ds)\n",
    "            print(\"Epoch {} complete, acc {}\".format(j, acc))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        x, y = mini_batch\n",
    "        nabla_b, nabla_w = self.backprop(x, y)\n",
    "        self.weights = [w-(eta/x.shape[0])*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/x.shape[0])*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [activation] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #print(\"activation {}, w {}, b {}\".format(activation.shape, w.shape, b.shape))\n",
    "            w = np.expand_dims(w, axis=0)\n",
    "            z = np.matmul(w, activation)+b\n",
    "            #print(\"z {}\".format(z.shape))\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        #print(\"y_pred\", activations[-1].shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        #print(\"accuracy {}\".format(accuracy(activations[-1], y)))\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        #print(\"delta {}, act {}\".format(delta.shape, activations[-2].transpose(0, 2, 1).shape))\n",
    "        #print(\"nabla_w[-1] {}\".format(nabla_w[-1].shape))\n",
    "        nabla_b[-1] = np.sum(delta, axis=0)\n",
    "        tmp_w = np.matmul(delta, activations[-2].transpose(0, 2, 1))\n",
    "        nabla_w[-1] = np.sum(tmp_w, axis=0)\n",
    "        #print(\"nabla_w[-1] {}\".format(nabla_w[-1].shape))\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            #\n",
    "            w = self.weights[-l+1].transpose()\n",
    "            w = np.expand_dims(w, axis=0)\n",
    "            #print(\"delta {}, w {}\".format(delta.shape, w.shape))\n",
    "            delta = np.matmul(w, delta) * sp\n",
    "            #print(\"delta {}, z {}, w {}\".format(delta.shape, z.shape, self.weights[-l+1].transpose().shape))\n",
    "            #print(\"act {}\".format(activations[-l-1].transpose().shape))\n",
    "            nabla_b[-l] = np.sum(delta, axis=0)\n",
    "            tmp_w = np.matmul(delta, activations[-l-1].transpose(0, 2, 1))\n",
    "            nabla_w[-l] = np.sum(tmp_w, axis=0)\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_ds):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = []\n",
    "        for i, data in zip(range(len(test_ds)), test_ds()):\n",
    "            x, y = data\n",
    "            y_pred = self.feedforward(x)\n",
    "            tmp = accuracy(y_pred, y)\n",
    "            test_results.append(tmp)\n",
    "        return np.mean(test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives partial C_x\n",
    "        partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgxuOFmVI9Q0"
   },
   "outputs": [],
   "source": [
    "net = Network([784, 100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6vcmdM0JBFB",
    "outputId": "439dfc95-6b95-4876-85be-6825680adf87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete, acc 0.26316666666666666\n",
      "Epoch 1 complete, acc 0.2680666666666667\n",
      "Epoch 2 complete, acc 0.2735\n",
      "Epoch 3 complete, acc 0.28055\n",
      "Epoch 4 complete, acc 0.2886666666666667\n"
     ]
    }
   ],
   "source": [
    "net.SGD(train_loader, epochs=5, eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkVpcRRmOJBO",
    "outputId": "298f55ec-804d-473f-c844-bf37cfbd5334"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.666666666666667e-05)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_hvTDZ2AFl5n"
   },
   "source": [
    "# Build Model by layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keqXFkoIFrcM"
   },
   "source": [
    "## Build Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzE58txdF1cY"
   },
   "source": [
    "### Build layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KyEhn3PHFt_0"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, name=\"layer\"):\n",
    "        self.name = name\n",
    "        self.__parameters = []\n",
    "        self.__grads = []\n",
    "        self.__is_derivable = False\n",
    "\n",
    "    def set_is_derivable(self, bVal):\n",
    "        self.__is_derivable = bVal\n",
    "\n",
    "    def is_derivable(self):\n",
    "        return self.__is_derivable\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.__grads\n",
    "\n",
    "    def get_grad(self, arg:int):\n",
    "        return self.__grads[arg]\n",
    "\n",
    "    def _init_param(self, shape, init_fn):\n",
    "        if (init_fn is not None):\n",
    "            x = init_fn(shape)\n",
    "        else:\n",
    "            # init like glorot uniform\n",
    "            lim = np.sqrt(6/np.sum(shape))\n",
    "            x = np.random.uniform(low=-lim, high=lim, size=shape)\n",
    "        self.__parameters.append(x)\n",
    "        if (not self.__is_derivable):\n",
    "            self.__grads.append(np.zeros(shape, dtype=np.float32))\n",
    "        else:\n",
    "            self.__grads = None\n",
    "        return x\n",
    "\n",
    "    def backward(self, x):\n",
    "        raise NameError(\"Layer {}: The method 'backward' is not implemented\".format(self.name))\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.__parameters\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NameError(\"Layer {}: The method '__call__' is not implemented\".format(self.name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VT4KDPr4GfyC"
   },
   "source": [
    "### Build Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MaWghQiGGg-9"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_size, out_size, init_fn=None, use_bias=False, init_fn_b=None, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__use_bias = use_bias\n",
    "        self.weight = self._init_param((out_size, in_size), init_fn)\n",
    "        if (self.__use_bias):\n",
    "            self.bias = self._init_param((out_size, 1), init_fn_b)\n",
    "        self.set_is_derivable(False)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        # get batch size\n",
    "        #batch_size = features.shape[0]\n",
    "        # calculate weight gradients\n",
    "        tmp_w  = np.matmul(delta, features.transpose(0, 2, 1))# D*Ft\n",
    "        grad_w = self.get_grad(0)\n",
    "        np.sum(tmp_w, axis=0, out=grad_w)# w = D*Ft\n",
    "        #print(\"---start ID grad_w {}\".format(id(grad_w)))\n",
    "        #tmp = grad_w.copy()\n",
    "        #print(\"----grad_w {}, equal {}\".format(grad_w.shape, np.allclose(tmp, grad_w)))\n",
    "        # calculate bias gradients\n",
    "        if (self.__use_bias):\n",
    "            grad_b = self.get_grad(1)\n",
    "            np.sum(delta, axis=0, out=grad_b)\n",
    "            #print(\"----grad_b\", grad_b.shape)\n",
    "        del tmp_w\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weight\n",
    "\n",
    "    def __call__(self, x):\n",
    "        w = np.expand_dims(self.weight, axis=0)\n",
    "        x = np.matmul(w, x)\n",
    "        if (self.__use_bias):\n",
    "            x += self.bias\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx2qGwymsqbO"
   },
   "outputs": [],
   "source": [
    "l_dense = Dense(10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tC6c9svysybY",
    "outputId": "587d48d9-c8ca-466e-afd0-a21b14f3894d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(l_dense.weight) == id(l_dense.parameters()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQWzBt3zHykG"
   },
   "source": [
    "### Build Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lPoMmDIgHwis"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self, min=0, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__min = min\n",
    "        self.set_is_derivable(True)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        self.set_grads(None)\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return (features > 0).astype(np.float32)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.maximum(x, self.__min)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ly53MHbvIOEg"
   },
   "source": [
    "### Build Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cpHlU_iDIMTQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__t = 1.\n",
    "        self.set_is_derivable(True)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        pass\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        # Suppose softmax over channel dim\n",
    "        x = self(features)\n",
    "        x = x*(self.__t-x)\n",
    "        return x\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = 1.0/(1.0+np.exp(-x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lf0sWCYFIx71"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oD6sHmhfI28z"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class NetworkLayers(object):\n",
    "\n",
    "    def __init__(self, layers, act_layers):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.layers     = layers\n",
    "        self.act_layers = act_layers\n",
    "        self.num_layers = len(self.layers)+1\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for layer, act_layer in zip(self.layers, self.act_layers):\n",
    "            a = layer(a)\n",
    "            a = act_layer(a)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, train_ds, epochs, eta=0.001):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        for j in range(epochs):\n",
    "            for i, data in zip(range(len(train_ds)), train_ds()):\n",
    "                self.update_mini_batch(data, eta)\n",
    "            acc = self.evaluate(train_ds)\n",
    "            print(\"Epoch {} complete, acc {}\".format(j, acc))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        x, y = mini_batch\n",
    "        self.backprop(x, y)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            l_parameters = layer.parameters()\n",
    "            l_grads = layer.get_grads()\n",
    "            if (l_grads is not None):\n",
    "                for parameters, grads in zip(l_parameters, l_grads):\n",
    "                    #print(\"---start ID parameters {}\".format(id(parameters)))\n",
    "                    #print(\"---start ID grads {}\".format(id(grads)))\n",
    "                    parameters -= (eta/x.shape[0])*grads\n",
    "                    #print(\"+++end ID parameters {}\".format(id(parameters)))\n",
    "\n",
    "    def virtual_backward(self, data):\n",
    "        for x, y in data:\n",
    "            # feedforward\n",
    "            activation = x\n",
    "            activations = [activation] # list to store all the activations, layer by layer\n",
    "            zs = [] # list to store all the z vectors, layer by layer\n",
    "            for layer, act_layer in zip(self.layers, self.act_layers):\n",
    "                z = layer(activation)\n",
    "                activation = act_layer(z)\n",
    "                #print(\"activation {}, w {}, b {}\".format(activation.shape, w.shape, b.shape))\n",
    "                #print(\"z {}\".format(z.shape))\n",
    "                zs.append(z)\n",
    "                activations.append(activation)\n",
    "            # backward pass\n",
    "            #print(\"y_pred\", activations[-1].shape)\n",
    "            #print(\"y\", y.shape)\n",
    "            #print(\"accuracy {}\".format(accuracy(activations[-1], y)))\n",
    "            graph_execution = []\n",
    "            layer     = self.layers[-1]\n",
    "            act_layer = self.act_layers[-1]\n",
    "            graph_execution.append({\"layer\":(layer.name, -1)})\n",
    "            delta = self.cost_derivative(activations[-1], y) * act_layer.get_prime(zs[-1])\n",
    "            graph_execution[-1].update({\"Wt\":None})\n",
    "            graph_execution[-1].update({\"prime\":(act_layer.name, -1)})\n",
    "            #print(\"delta {}, act {}\".format(delta.shape, activations[-2].transpose(0, 2, 1).shape))\n",
    "            layer.backward(delta, activations[-2])\n",
    "            graph_execution[-1].update({\"update_params\":(self.act_layers[-2].name, -2)})\n",
    "            # Note that the variable l in the loop below is used a little\n",
    "            # differently to the notation in Chapter 2 of the book.  Here,\n",
    "            # l = 1 means the last layer of neurons, l = 2 is the\n",
    "            # second-last layer, and so on.  It's a renumbering of the\n",
    "            # scheme in the book, used here to take advantage of the fact\n",
    "            # that Python can use negative indices in lists.\n",
    "            for l in range(2, self.num_layers):\n",
    "                layer     = self.layers[-l]\n",
    "                act_layer = self.act_layers[-l]\n",
    "                z = zs[-l]\n",
    "                sp = act_layer.get_prime(z)\n",
    "                #\n",
    "                w = self.layers[-l+1].get_weights().transpose()\n",
    "                w = np.expand_dims(w, axis=0)\n",
    "                #print(\"delta {}, w {}\".format(delta.shape, w.shape))\n",
    "                delta = np.matmul(w, delta) * sp\n",
    "                graph_execution[-1].update({\"Wt\":(self.__layers[-l+1].name, -l+1)})\n",
    "                graph_execution[-1].update({\"prime\":(act_layer.name, -l)})\n",
    "                #print(\"delta {}, z {}, w {}\".format(delta.shape, z.shape, self.weights[-l+1].transpose().shape))\n",
    "                #print(\"act {}\".format(activations[-l-1].transpose().shape))\n",
    "                layer.backward(delta, activations[-l-1])\n",
    "                if (self.num_layers-(l+1) >= 0):\n",
    "                    graph_execution[-1].update({\"update_params\":(self.act_layers[-l-1].name, -l-1)})\n",
    "                else:\n",
    "                    graph_execution[-1].update({\"update_params\":(\"Input\", -l-1)})\n",
    "            \n",
    "\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [activation] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for layer, act_layer in zip(self.layers, self.act_layers):\n",
    "            z = layer(activation)\n",
    "            activation = act_layer(z)\n",
    "            #print(\"activation {}, w {}, b {}\".format(activation.shape, w.shape, b.shape))\n",
    "            #print(\"z {}\".format(z.shape))\n",
    "            zs.append(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        #print(\"y_pred\", activations[-1].shape)\n",
    "        #print(\"y\", y.shape)\n",
    "        #print(\"accuracy {}\".format(accuracy(activations[-1], y)))\n",
    "        layer     = self.layers[-1]\n",
    "        act_layer = self.act_layers[-1]\n",
    "        delta = self.cost_derivative(activations[-1], y) * act_layer.get_prime(zs[-1])\n",
    "        #print(\"delta {}, act {}\".format(delta.shape, activations[-2].transpose(0, 2, 1).shape))\n",
    "        layer.backward(delta, activations[-2])\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            layer     = self.layers[-l]\n",
    "            act_layer = self.act_layers[-l]\n",
    "            z = zs[-l]\n",
    "            sp = act_layer.get_prime(z)\n",
    "            #\n",
    "            w = self.layers[-l+1].get_weights().transpose()\n",
    "            w = np.expand_dims(w, axis=0)\n",
    "            #print(\"delta {}, w {}\".format(delta.shape, w.shape))\n",
    "            delta = np.matmul(w, delta) * sp\n",
    "            #print(\"delta {}, z {}, w {}\".format(delta.shape, z.shape, self.weights[-l+1].transpose().shape))\n",
    "            #print(\"act {}\".format(activations[-l-1].transpose().shape))\n",
    "            layer.backward(delta, activations[-l-1])\n",
    "\n",
    "    def evaluate(self, test_ds):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = []\n",
    "        for i, data in zip(range(len(test_ds)), test_ds()):\n",
    "            x, y = data\n",
    "            y_pred = self.feedforward(x)\n",
    "            tmp = accuracy(y_pred, y)\n",
    "            test_results.append(tmp)\n",
    "        return np.mean(test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives partial C_x\n",
    "        partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IzpE5CUoqKYm"
   },
   "outputs": [],
   "source": [
    "\n",
    "init_fn = lambda shape: np.random.randn(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jd_A_3iIqMGD",
    "outputId": "6a56ccb7-0b23-454b-f4cd-258070a03397"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26707612, -0.20242758]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_fn((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vco0NEMLZMMi"
   },
   "outputs": [],
   "source": [
    "\n",
    "layers     = [\n",
    "    Dense(784, 100, init_fn=init_fn, use_bias=True, init_fn_b=init_fn, name=\"Dense_h1\"),\n",
    "    Dense(100, 10, init_fn=init_fn, use_bias=True, init_fn_b=init_fn, name=\"Dense_h2\"),\n",
    "    ]\n",
    "act_layers = [\n",
    "    Sigmoid(name=\"Sigmoid_h1\"),\n",
    "    #Relu(name=\"Relu_h1\"),\n",
    "    Sigmoid(name=\"Sigmoid_h2\"),\n",
    "    ]\n",
    "netLayers = NetworkLayers(layers, act_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTPbUD5bZP6h",
    "outputId": "90bf7c4b-99c1-46dc-c1bd-011ce8e72ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete, acc 0.23431666666666667\n",
      "Epoch 1 complete, acc 0.2582\n"
     ]
    }
   ],
   "source": [
    "netLayers.SGD(train_loader, epochs=20, eta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ol8b7CmaF1TN"
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "x = np.random.uniform(low=-1, high=1, size=(batch_size, 784, 1))\n",
    "w = np.random.uniform(low=-1, high=1, size=(1, 100, 784))\n",
    "b_y = np.matmul(w, x)\n",
    "y = np.dot(w[0], x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tzZyUpTOH31y",
    "outputId": "fe66c3c1-5c5f-4c64-dfef-5223cb9f4929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(b_y[0], y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c_7PJiEjE1oQ",
    "bkXtbXsYE83A",
    "VUAnrlLTE_bX",
    "69vODF9qFC1K",
    "XFM2SE0RFK4R",
    "zdB1TO4bFROP",
    "mK8ixK8NFT1H",
    "vrv0gwHHFcU6",
    "TQWzBt3zHykG",
    "ly53MHbvIOEg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
