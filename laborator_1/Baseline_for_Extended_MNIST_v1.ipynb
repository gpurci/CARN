{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x55A6JvfHaAK"
   },
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TG8McvlDHd8u",
    "outputId": "a2392b63-db7b-4cc3-f81e-c00be9192ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting livelossplot\n",
      "  Downloading livelossplot-0.5.6-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from livelossplot) (3.10.0)\n",
      "Requirement already satisfied: bokeh in /usr/local/lib/python3.12/dist-packages (from livelossplot) (3.7.3)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (3.1.6)\n",
      "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (1.3.3)\n",
      "Requirement already satisfied: narwhals>=1.13 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (2.0.2)\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (25.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (2.2.2)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (11.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (6.0.3)\n",
      "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (6.4.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh->livelossplot) (2025.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->livelossplot) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->livelossplot) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->livelossplot) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->livelossplot) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.17.0)\n",
      "Downloading livelossplot-0.5.6-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: livelossplot\n",
      "Successfully installed livelossplot-0.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xQA0TbS9iJz"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JnlB_3F-4Ypz"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "# You are not allowed to use DataLoader, torch.nn, torch.functional, torchvision\n",
    "# Backpropagation and data preprocessing must be implemented from scratch\n",
    "# Check https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/how_to/use_kaggle.md for learning how to use kaggle and submit to this competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MavX5UXd_jqX"
   },
   "source": [
    "# Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-gsQ54N4Hy2Y"
   },
   "source": [
    "## Preprocesing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a3sb7JXDHzWh"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normalization_n1_p1(x):\n",
    "    return (x/127.5)-1\n",
    "\n",
    "def normalization_n0_p1(x):\n",
    "    return x/255\n",
    "\n",
    "def one_hot(y, num_classes=1):\n",
    "    #print(y.shape, y)\n",
    "    tmp = torch.zeros((y.size(0), num_classes), dtype=torch.float32)\n",
    "    tmp[torch.arange(y.size(0)), y] = 1.0\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yp35IrNCH1ED"
   },
   "source": [
    "## Summary function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "O8ajvvlBH6TJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_dataset_fn(img, label):\n",
    "    info_str = \"\"\"img: shape {}, min {}, max {}, type {};\n",
    "label: shape {}, min {}, max {}, type {};\"\"\".format(img.shape, torch.min(img), torch.max(img), type(img),\n",
    "                                           label.shape, torch.min(label), torch.max(label), type(label))\n",
    "    print(info_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6JV4BsOHsBz"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JPQpvpRF_mM3"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ExtendedMNISTDataset(Dataset):\n",
    "    def __init__(self, root: str = \"/kaggle/input/fii-atnn-2025-competition-1\", train: bool = True):\n",
    "        \"\"\"ExtendedMNISTDataset:\n",
    "        root  - path root of dataset\n",
    "        train - read train or test dataset (true - train, false - test)\n",
    "        preprocessing - image preprocesing function\n",
    "        Dataset structure:\n",
    "            list -> (batchsize, tuple -> (np.array -> (image->(784, )), scalar -> (labels->1)))\n",
    "        \"\"\"\n",
    "        # select filename\n",
    "        if (train):\n",
    "            file = \"extended_mnist_train.pkl\"\n",
    "        else:\n",
    "            file = \"extended_mnist_test.pkl\"\n",
    "        # join root to filename\n",
    "        filename = os.path.join(root, file)\n",
    "        # read dataset\n",
    "        dataset = self.__read(filename)\n",
    "        self.inputs, self.outputs = self.__split_data(dataset)\n",
    "\n",
    "    def __read(self, filename):\n",
    "        # try to open filename\n",
    "        try:\n",
    "            f = open(filename, \"rb\")\n",
    "            try:\n",
    "                dataset = pickle.load(f)\n",
    "            except Exception as e:\n",
    "                dataset = None\n",
    "                self.__show_exception(e)\n",
    "            finally:\n",
    "                f.close()\n",
    "        except IOError as e:\n",
    "            dataset = None\n",
    "            self.__show_exception(e)\n",
    "        return dataset\n",
    "\n",
    "    def __show_exception(self, e) -> None:\n",
    "        tb = traceback.extract_tb(e.__traceback__)\n",
    "        last_call = tb[-1]\n",
    "        print(f\"❌ Error in function '{last_call.name}' at line {last_call.lineno}\")\n",
    "        print(f\"   File: {last_call.filename}\")\n",
    "        print(f\"   Exception: {e}\")\n",
    "\n",
    "    def __split_data(self, dataset):\n",
    "        inputs  = []\n",
    "        outputs = []\n",
    "        for input, ouput in dataset:\n",
    "            inputs.append(input)\n",
    "            outputs.append(ouput)\n",
    "        return np.array(inputs, dtype=np.uint8), np.array(outputs, dtype=np.int32)\n",
    "\n",
    "    def __len__(self, ) -> int:\n",
    "        return self.inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, i : int):# int|np array\n",
    "        return self.inputs[i], self.outputs[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "s0FzmsLLAw8Y"
   },
   "outputs": [],
   "source": [
    "#\n",
    "train_ds = ExtendedMNISTDataset(root=\"/content\", train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqLrmx1JpUVq",
    "outputId": "f19073b7-2f3f-4263-f028-93687a9b1319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=uint8),\n",
       " array([2, 6, 2], dtype=int32))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RHe3TjK9nCSB",
    "outputId": "08a3f47b-6e83-437b-d05d-bc63ef835e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "HC5GijsbnFdW"
   },
   "outputs": [],
   "source": [
    "_, y = train_ds[np.arange(len(train_ds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jl_Zz0uhndHc",
    "outputId": "61ce7564-fa99-4298-a11a-c2a5f361917b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int32(9)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9nd7JEAH2aC"
   },
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TkXn5Zz1AwF6"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, dataset, batchsize=1, shuffle=False):\n",
    "        assert (batchsize > 0), \"batchsize should be ghreat than 'zero'\"\n",
    "        assert (isinstance(shuffle, bool)), \"shuffle should be 'bool'\"\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.shuffle = shuffle\n",
    "        self.size = len(self.dataset)\n",
    "        # shufle\n",
    "        if (self.shuffle):\n",
    "            self.permutation = np.random.permutation(self.size)\n",
    "        else:\n",
    "            self.permutation = np.arange(self.size, dtype=np.int32)\n",
    "        self.maps_fn = []\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)//self.batchsize\n",
    "\n",
    "    def __call__(self):\n",
    "        try:\n",
    "            for i in range(0, self.size, self.batchsize):\n",
    "                pos  = self.permutation[i:i+self.batchsize]\n",
    "                datas = self.dataset[pos]\n",
    "                if (isinstance(datas, tuple)):\n",
    "                    datas = [torch.from_numpy(data) for data in datas]\n",
    "                else:\n",
    "                    datas = torch.from_numpy(datas)\n",
    "                datas = self.__map_fn(datas)\n",
    "                yield datas\n",
    "            else:\n",
    "                if (self.shuffle):\n",
    "                    self.permutation = np.random.permutation(self.size)\n",
    "        except Exception as e:\n",
    "            self.__show_exception(e)\n",
    "\n",
    "    def take(self, size):\n",
    "        try:\n",
    "            for i, data in zip(range(size), self()):\n",
    "                yield data\n",
    "            else:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            self.__show_exception(e)\n",
    "\n",
    "    def __show_exception(self, e) -> None:\n",
    "        tb = traceback.extract_tb(e.__traceback__)\n",
    "        last_call = tb[-1]\n",
    "        print(f\"❌ Error in function '{last_call.name}' at line {last_call.lineno}\")\n",
    "        print(f\"   File: {last_call.filename}\")\n",
    "        print(f\"   Exception: {e}\")\n",
    "\n",
    "    def map(self, fn):\n",
    "        self.maps_fn.append(fn)\n",
    "\n",
    "    def __map_fn(self, data):\n",
    "        for fn in self.maps_fn:\n",
    "            data = fn(*data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "d144IRutFDMJ"
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(train_ds, batchsize=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "40WpDif0GcuQ"
   },
   "outputs": [],
   "source": [
    "dataloader.map(lambda x, y: (normalization_n1_p1(x), one_hot(y, num_classes=10)))\n",
    "dataloader.map(lambda x, y: (x.unsqueeze(-1), y.unsqueeze(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7uZwjchzB8SQ",
    "outputId": "9f1f77cc-c75d-4a58-c1a6-bb9cfb7030e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "img: shape torch.Size([20, 784, 1]), min -1.0, max 1.0, type <class 'torch.Tensor'>;\n",
      "label: shape torch.Size([20, 10, 1]), min 0.0, max 1.0, type <class 'torch.Tensor'>;\n"
     ]
    }
   ],
   "source": [
    "for img, label in dataloader.take(100):\n",
    "    test_dataset_fn(img, label)\n",
    "    #print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8aGDKmc9tzF"
   },
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jCJPk8PR4YmX"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.\n",
    "\"\"\"\n",
    "\n",
    "class SGD(object):\n",
    "\n",
    "    def __init__(self, parameters, lr):\n",
    "        \"\"\"\n",
    "        parameters - a list of parameters for every layer\n",
    "                    -> list of parameters\n",
    "                        -> one element of parameter is list of torch tensor\n",
    "        lr - learning rate\n",
    "        \"\"\"\n",
    "        self.__parameters = parameters\n",
    "        self.__lr = lr\n",
    "\n",
    "    def __call__(self, grads):\n",
    "        \"\"\"\n",
    "        grads - a list of grads for every layer\n",
    "                    -> list of grads for parameters\n",
    "                        -> one element of grads is list of torch tensor\n",
    "        \"\"\"\n",
    "        #print(\"____optimizer____\")\n",
    "        #print(\"list layers: parameter {}, size {}, grad {}, size {}\".format(type(self.__parameters), len(self.__parameters), type(grads), len(grads)))\n",
    "        for l_parameters, l_grads in zip(self.__parameters, grads):\n",
    "            #print(\"list: parameters {}, grads {}\".format(type(l_parameters), type(l_grads)))\n",
    "            if ((l_grads is not None)):\n",
    "                for parameter, grad in zip(l_parameters, l_grads):\n",
    "                    #print(\"parameter {}, grad {}\".format(parameter.shape, grad.shape))\n",
    "                    #batch_size = grad.size(0)\n",
    "                    #grad = grad.sum(dim=0, keepdim=False)\n",
    "                    #grad = torch.div(grad, batch_size, out=grad)\n",
    "                    #print(\"parameter {}, grad {}\".format(parameter.shape, grad.shape))\n",
    "                    torch.sub(parameter, grad, alpha=self.__lr, out=parameter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVslqY8VHz_V"
   },
   "source": [
    "# Build Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AW3g5UadSVzk"
   },
   "source": [
    "## Build MetricsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Z4RkFcnpH2RN"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MetricsList(object):\n",
    "    def __init__(self, metrics_fn):\n",
    "        \"\"\"\"\"\"\n",
    "        if (metrics_fn is not None):\n",
    "            if (isinstance(metrics_fn, (dict))):\n",
    "                for key in metrics_fn.keys():\n",
    "                    if (not isinstance(metrics_fn[key], Metrics)):\n",
    "                        raise NameError(\"The metric: '{}' is type '{}' not as 'Metrics' object\".format(key, type(metrics_fn[key])))\n",
    "            else:\n",
    "                raise NameError(\"The argument need to be as 'Dict' object, but is '{}'\".format(type(metrics_fn)))\n",
    "        else:\n",
    "            pass\n",
    "        #\n",
    "        self.__metrics_fn = metrics_fn\n",
    "\n",
    "    def __call__(self, y, y_pred):\n",
    "        logs = {}\n",
    "        if (self.__metrics_fn is not None):\n",
    "            for key in self.__metrics_fn.keys():\n",
    "                metric = self.__metrics_fn[key](y, y_pred)\n",
    "                logs[key] = metric\n",
    "        return logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGAwM2WMSaxa"
   },
   "source": [
    "## Build Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UKHTFByoSdJc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Metrics(object):\n",
    "    def __init__(self, name=\"\"):\n",
    "        \"\"\"\"\"\"\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, y_pred, y):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2xIZWnAnU41V"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Acuracy(Metrics):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        pass\n",
    "\n",
    "    def __call__(self, y_pred, y):\n",
    "        # y_pred: logits or probabilities (batch_size, num_classes)\n",
    "        # y_true: true labels (batch_size,)\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        y     = torch.argmax(y,      dim=1)\n",
    "        correct = (preds == y).sum().item()\n",
    "        total = y.size(0)\n",
    "        return correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5ucXnc8H2l_"
   },
   "source": [
    "# Build Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdk7L7ObSgPJ"
   },
   "source": [
    "## Build CallbacksList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "iE4K8V6aH8Pk"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CallbacksList(object):\n",
    "    def __init__(self, callbacks):\n",
    "        \"\"\"\"\"\"\n",
    "        if (callbacks is not None):\n",
    "            if (isinstance(callbacks, list)):\n",
    "                    for i, callback in enumerate(callbacks, 0):\n",
    "                        if (not isinstance(callback, (type(Callbacks()), type(PlotLossesKeras())))):\n",
    "                            raise NameError(\"The callback: '{}' is type '{}' not as 'Callbacks' object\".format(i, type(callback)))\n",
    "            else:\n",
    "                raise NameError(\"The argument need to be as 'List' object, but is '{}'\".format(type(callbacks)))\n",
    "        else:\n",
    "            callbacks = []\n",
    "        #\n",
    "        self.__callbacks = callbacks\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_batch_begin(batch, logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_batch_end(batch, logs)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_epoch_begin(epoch, logs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_predict_batch_begin(batch, logs)\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_predict_batch_end(batch, logs)\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_predict_begin(logs)\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_predict_end(logs)\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_test_batch_begin(batch, logs)\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_test_batch_end(batch, logs)\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_test_begin(logs)\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_test_end(logs)\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_train_batch_begin(batch, logs)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_train_batch_end(batch, logs)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_train_begin(logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for callback in self.__callbacks:\n",
    "            callback.on_train_end(logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC-Fusm36iXQ"
   },
   "source": [
    "## Build Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "t3wLdybR6ltJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Callbacks(object):\n",
    "    def __init__(self,):\n",
    "        \"\"\"\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wr49EeH1m_G"
   },
   "source": [
    "# Build Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCNgANQO5sAu"
   },
   "source": [
    "## Build Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Dda0Ro4t5uZe"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, name=\"layer\"):\n",
    "        torch.set_grad_enabled(False) # do not use pythorch gradient\n",
    "        self.name = name\n",
    "        self.__parameters = []\n",
    "        self.__grads = None\n",
    "        self.__is_derivable = False\n",
    "\n",
    "    def set_is_derivable(self, bVal):\n",
    "        self.__is_derivable = bVal\n",
    "\n",
    "    def set_grads(self, grads):\n",
    "        self.__grads = grads\n",
    "\n",
    "    def is_derivable(self):\n",
    "        return self.__is_derivable\n",
    "\n",
    "    def get_grads(self):\n",
    "        return self.__grads\n",
    "\n",
    "    def _init_param(self, shape, init_fn):\n",
    "        if (init_fn is not None):\n",
    "            x = init_fn(shape)\n",
    "        else:\n",
    "            # init like glorot uniform\n",
    "            lim = np.sqrt(6/np.sum(shape))\n",
    "            x = torch.empty(*shape).uniform_(-lim, lim)\n",
    "        self.__parameters.append(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, x):\n",
    "        raise NameError(\"Layer {}: The method 'backward' is not implemented\".format(self.name))\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.__parameters\n",
    "\n",
    "    def copy(self):\n",
    "        copy_params = []\n",
    "        for param in self.__parameters:\n",
    "            c_param = torch.empty(param.size(), dtype=torch.float32)\n",
    "            c_param.copy_(param, non_blocking=False)\n",
    "            copy_params.append(c_param)\n",
    "        return copy_params\n",
    "\n",
    "    def cmp(self, parameters: list):\n",
    "        is_equal = True\n",
    "        print(\"layer {}, size: loc {}, par {}\".format(self.name, len(self.__parameters), len(parameters)))\n",
    "        for loc_parameters, c_parameters in zip(self.__parameters, parameters):\n",
    "            is_equal = is_equal and torch.allclose(loc_parameters, c_parameters)\n",
    "        return is_equal\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NameError(\"Layer {}: The method '__call__' is not implemented\".format(self.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "CNS-o5cc697h"
   },
   "outputs": [],
   "source": [
    "test_l = Layer(name=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STQxIDrfjGhm"
   },
   "source": [
    "## Build Input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eivNe4zujJtZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.set_is_derivable(False)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        self.set_grads(None)\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkCJdVkn1vwi"
   },
   "source": [
    "## Build Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aiP-o9mjbihY"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_size, out_size, init_fn=None, use_bias=False, init_fn_b=None, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__use_bias = use_bias\n",
    "        self.__w = self._init_param((out_size, in_size), init_fn)\n",
    "        if (self.__use_bias):\n",
    "            self.__b = self._init_param((out_size, 1), init_fn_b)\n",
    "        self.set_is_derivable(False)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        # get batch size\n",
    "        batch_size = features.size(0)\n",
    "        # calculate bias gradients\n",
    "        if (self.__use_bias):\n",
    "            nabla_b = delta.sum(dim=0)\n",
    "            torch.div(nabla_b, batch_size, out=nabla_b)\n",
    "            #print(\"----nabla_b\", nabla_b.shape)\n",
    "        # perform dimensions\n",
    "        features = features.transpose(-1, -2)\n",
    "        #print(\"----delta\", delta.shape)\n",
    "        #print(\"----features\", features.shape)\n",
    "        # calculate weight gradients\n",
    "        nabla_w = torch.matmul(delta, features)# w = D*Ft\n",
    "        grad_w = nabla_w.sum(dim=0)\n",
    "        torch.div(grad_w, batch_size, out=grad_w)\n",
    "        #print(\"----nabla_w\", nabla_w.shape)\n",
    "        # set gradients\n",
    "        grads = [grad_w]\n",
    "        if (self.__use_bias):\n",
    "            grads.append(nabla_b)\n",
    "        self.set_grads(grads)\n",
    "        del nabla_w\n",
    "        # Empty cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.__w\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.matmul(self.__w.unsqueeze(0), x)\n",
    "        if (self.__use_bias):\n",
    "            x = torch.add(x, self.__b)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV9MIHyIb72l",
    "outputId": "4f09dd64-76ab-4f26-a6fa-c4e42f4f5997"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 2\n",
    "delta    = torch.empty(batch_size, 10, 1).uniform_(-1, 1)\n",
    "features = torch.empty(batch_size, 100, 1).uniform_(-1, 1)\n",
    "\n",
    "features = features.transpose(-1, -2)\n",
    "# calculate weight gradients\n",
    "torch.matmul(delta, features).sum(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "foo3FM-YepXe",
    "outputId": "1cae46a2-5143-46a0-afd9-3ac4eafee169"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 1])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "batch_size = 2\n",
    "w = torch.empty(batch_size, 100, 784).uniform_(-1, 1)\n",
    "x = torch.empty(batch_size, 784, 1).uniform_(-1, 1)\n",
    "\n",
    "# calculate weight gradients\n",
    "torch.matmul(w, x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "yHV4lyJkPRTw"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, in_size, out_size, init_fn=None, use_bias=False, init_fn_b=None, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__use_bias = use_bias\n",
    "        self.__w = self._init_param((in_size, out_size), init_fn)\n",
    "        if (self.__use_bias):\n",
    "            self.__b = self._init_param((out_size, ), init_fn_b)\n",
    "        self.set_is_derivable(False)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        # get batch size\n",
    "        batch_size = features.size(0)\n",
    "        # calculate bias gradients\n",
    "        if (self.__use_bias):\n",
    "            nabla_b = delta.sum(dim=0)\n",
    "            torch.div(nabla_b, batch_size, out=nabla_b)\n",
    "            #print(\"----nabla_b\", nabla_b.shape)\n",
    "        # perform dimensions\n",
    "        features = features.transpose(1, 0)\n",
    "        #print(\"----delta\", delta.shape)\n",
    "        #print(\"----features\", features.shape)\n",
    "        # calculate weight gradients\n",
    "        nabla_w = torch.matmul(features, delta)# w = Ft*D\n",
    "        torch.div(nabla_w, batch_size, out=nabla_w)\n",
    "        #print(\"----nabla_w\", nabla_w.shape)\n",
    "        # set gradients\n",
    "        grads = [nabla_w]\n",
    "        if (self.__use_bias):\n",
    "            grads.append(nabla_b)\n",
    "        self.set_grads(grads)\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return None\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.__w\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.matmul(x, self.__w)\n",
    "        if (self.__use_bias):\n",
    "            x = torch.add(x, self.__b)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "xf6jp434lCfp",
    "outputId": "48735a66-5c4c-4cf2-cdda-459af81eebf7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\ndef backward(self, delta, features):\\n    # get batch size\\n    batch_size = features.size(0)\\n    # calculate nabla for bias\\n    if (self.__use_bias):\\n        nabla_b = delta.sum(dim=0)\\n        torch.div(nabla_b, batch_size, out=nabla_b)\\n        #print(\"----nabla_b\", nabla_b.shape)\\n    # prepare dimensions\\n    delta    = delta.unsqueeze(1)\\n    features = features.unsqueeze(-1)\\n    #print(\"----delta\", delta.shape)\\n    #print(\"----features\", features.shape)\\n    \\n    nabla_w = torch.bmm(features, delta)\\n    nabla_w = nabla_w.sum(dim=0)\\n    torch.div(nabla_w, batch_size, out=nabla_w)\\n    #print(\"----nabla_w\", nabla_w.shape)\\n    grads = [nabla_w]\\n    if (self.__use_bias):\\n        grads.append(nabla_b)\\n    self.set_grads(grads)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    def backward(self, delta, features):\n",
    "        batch_size = features.size(0)\n",
    "        #delta    = delta.unsqueeze(1)\n",
    "        features = features.transpose(1, 0)\n",
    "        #print(\"----delta\", delta.shape)\n",
    "        #print(\"----features\", features.shape)\n",
    "\n",
    "        nabla_w = torch.matmul(features, delta)# w = Ft*D\n",
    "        torch.div(nabla_w, batch_size, out=nabla_w)\n",
    "        #print(\"----nabla_w\", nabla_w.shape)\n",
    "        nabla_b = delta.sum(dim=0)\n",
    "        torch.div(nabla_b, batch_size, out=nabla_b)\n",
    "        #print(\"----nabla_b\", nabla_b.shape)\n",
    "        grads = [nabla_w, nabla_b]\n",
    "        self.set_grads(grads)\n",
    "    \"\"\"\n",
    "    def backward(self, delta, features):\n",
    "        # get batch size\n",
    "        batch_size = features.size(0)\n",
    "        # calculate nabla for bias\n",
    "        if (self.__use_bias):\n",
    "            nabla_b = delta.sum(dim=0)\n",
    "            torch.div(nabla_b, batch_size, out=nabla_b)\n",
    "            #print(\"----nabla_b\", nabla_b.shape)\n",
    "        # prepare dimensions\n",
    "        delta    = delta.unsqueeze(1)\n",
    "        features = features.unsqueeze(-1)\n",
    "        #print(\"----delta\", delta.shape)\n",
    "        #print(\"----features\", features.shape)\n",
    "\n",
    "        nabla_w = torch.bmm(features, delta)\n",
    "        nabla_w = nabla_w.sum(dim=0)\n",
    "        torch.div(nabla_w, batch_size, out=nabla_w)\n",
    "        #print(\"----nabla_w\", nabla_w.shape)\n",
    "        grads = [nabla_w]\n",
    "        if (self.__use_bias):\n",
    "            grads.append(nabla_b)\n",
    "        self.set_grads(grads)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-G7YogWSi5r",
    "outputId": "2fc6f5a9-85e2-4f6d-965b-de1b71545871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "tmp_x = torch.empty(3, 20).uniform_(-1, 1)  # Uniform [-1, 1]\n",
    "tmp_x = torch.ones(2, 3, 20)  # Uniform [-1, 1]\n",
    "print(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "wOhpSQ1laMJ-"
   },
   "outputs": [],
   "source": [
    "dense_l = Dense(in_size=20, out_size=3, use_bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j-O0adhIa5PO",
    "outputId": "60ac5c98-b8f0-41f9-e71d-c4fb1296f7de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1257, -1.0865, -1.1252],\n",
       "         [ 1.1257, -1.0865, -1.1252],\n",
       "         [ 1.1257, -1.0865, -1.1252]],\n",
       "\n",
       "        [[ 1.1257, -1.0865, -1.1252],\n",
       "         [ 1.1257, -1.0865, -1.1252],\n",
       "         [ 1.1257, -1.0865, -1.1252]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_l(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "5vSJG4MJfhWS",
    "outputId": "f228bff3-508f-4d23-f4c0-f34ae9391e09"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dense.backward() missing 1 required positional argument: 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-312491865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdense_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Dense.backward() missing 1 required positional argument: 'features'"
     ]
    }
   ],
   "source": [
    "dense_l.backward(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUCepVut2W9u"
   },
   "source": [
    "## Build Relu layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IlG2IsMs2aoS"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Relu(Layer):\n",
    "    def __init__(self, min=0, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__min = torch.tensor(min)\n",
    "        self.set_is_derivable(True)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        self.set_grads(None)\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        return (features > 0).float()\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.maximum(x, self.__min)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQAhto3u4GHd",
    "outputId": "e0eb7d2a-3646-407b-b820-9fe6f4a40dd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2147,  0.7854,  0.6982,  0.7165,  0.9628, -0.0168, -0.7561, -0.0126,\n",
      "         -0.0143,  0.2169,  0.4241, -0.5057, -0.4185,  0.0529,  0.4176,  0.7627,\n",
      "          0.2612, -0.9293, -0.8287,  0.1271],\n",
      "        [ 0.0707,  0.9269, -0.8177,  0.3035, -0.4018,  0.1338, -0.4901, -0.1726,\n",
      "          0.5371, -0.8540, -0.5457,  0.8025, -0.3467, -0.2213, -0.4189, -0.5081,\n",
      "          0.8426,  0.9289,  0.8051,  0.2765],\n",
      "        [ 0.4647,  0.5800, -0.9863, -0.9222, -0.1540, -0.9986,  0.4406,  0.7460,\n",
      "          0.9229, -0.0899,  0.8097,  0.3678, -0.8543, -0.0504,  0.9877,  0.0411,\n",
      "          0.2094, -0.5094,  0.6172, -0.0490]])\n"
     ]
    }
   ],
   "source": [
    "tmp_x = torch.empty(3, 20).uniform_(-1, 1)  # Uniform [-1, 1]\n",
    "#tmp_x = torch.ones(2, 3, 20)  # Uniform [-1, 1]\n",
    "print(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "sEVNXQ-M4qqG"
   },
   "outputs": [],
   "source": [
    "relu_l = Relu(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Fxqjc074wYh",
    "outputId": "4993a18c-5a38-46e9-d679-6b57bab57836"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_l(tmp_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "6c66vNb34_1D",
    "outputId": "abe241c0-5ede-46de-bf39-90e9d82eec15"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Relu.get_prime() missing 1 required positional argument: 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3436263032.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelu_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Relu.get_prime() missing 1 required positional argument: 'features'"
     ]
    }
   ],
   "source": [
    "relu_l.get_prime().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OVh9YZo02Fl"
   },
   "source": [
    "## Build softmax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5fQJ6gc607ig"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Softmax(Layer):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.set_is_derivable(True)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        self.set_grads(None)\n",
    "    \"\"\"\n",
    "    def get_prime(self, features):\n",
    "        # Suppose softmax over channel dim\n",
    "        x = self(features)\n",
    "        x_flat = x.reshape(-1, x.shape[-1])  # collapse batch*H*W into one dimension\n",
    "        # batch matrix multiplication\n",
    "        x_bmm = torch.bmm(x_flat.unsqueeze(2), x_flat.unsqueeze(1))\n",
    "        jacobian = torch.diag_embed(x_flat) - x_bmm\n",
    "        jacobian = jacobian.sum(dim=-1, keepdim=False)\n",
    "        return jacobian\n",
    "    \"\"\"\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        # Suppose softmax over channel dim\n",
    "        x = self(features)\n",
    "        diff = torch.sub(1, x, alpha=1, out=None)\n",
    "        x = torch.mul(x, diff, out=x)\n",
    "        return x\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.exp(x) - torch.max(x)\n",
    "        x = torch.div(x, torch.sum(x), rounding_mode=None, out=None)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-U3hComzdPl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99z6B2iC4bAD",
    "outputId": "07a9fbe2-ce17-48a2-ddf0-fbafc2409c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2362,  0.4914, -0.5989,  0.5817, -0.1990, -0.4743, -0.4052, -0.5721,\n",
      "          0.0346, -0.3257,  0.3997,  0.9869, -0.1369,  0.8468,  0.2440,  0.3187,\n",
      "         -0.3427, -0.2786, -0.9753, -0.3535],\n",
      "        [-0.1218,  0.8696,  0.6346,  0.7159, -0.5462, -0.9119,  0.3980, -0.8161,\n",
      "          0.9467,  0.3217,  0.0708,  0.0147, -0.8290,  0.4878,  0.3823, -0.3581,\n",
      "          0.4144,  0.1264,  0.9421,  0.5322],\n",
      "        [ 0.3252, -0.3648, -0.1405,  0.1632,  0.5896,  0.5241,  0.5790,  0.4439,\n",
      "          0.2877, -0.9869, -0.6017, -0.8320, -0.1324,  0.5844,  0.8483, -0.0297,\n",
      "         -0.5137, -0.4017, -0.3747, -0.6301]])\n"
     ]
    }
   ],
   "source": [
    "tmp_x = torch.empty(3, 20).uniform_(-1, 1)  # Uniform [-1, 1]\n",
    "#tmp_x = torch.ones(2, 3, 20)  # Uniform [-1, 1]\n",
    "print(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "O1UPSCAa4dOX"
   },
   "outputs": [],
   "source": [
    "softmax_l = Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EMg6M0e_4hNI",
    "outputId": "66582ad2-0bb1-4147-f2e6-0e6e11463ffb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0111, 0.0230, 0.0077, 0.0251, 0.0115, 0.0087, 0.0094, 0.0079, 0.0145,\n",
       "         0.0101, 0.0210, 0.0377, 0.0123, 0.0328, 0.0179, 0.0193, 0.0100, 0.0106,\n",
       "         0.0053, 0.0099],\n",
       "        [0.0124, 0.0335, 0.0265, 0.0287, 0.0081, 0.0056, 0.0209, 0.0062, 0.0362,\n",
       "         0.0194, 0.0151, 0.0143, 0.0061, 0.0229, 0.0206, 0.0098, 0.0213, 0.0159,\n",
       "         0.0360, 0.0239],\n",
       "        [0.0194, 0.0098, 0.0122, 0.0165, 0.0253, 0.0237, 0.0251, 0.0219, 0.0187,\n",
       "         0.0052, 0.0077, 0.0061, 0.0123, 0.0252, 0.0328, 0.0136, 0.0084, 0.0094,\n",
       "         0.0097, 0.0075]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_l(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyWu3Uqu4ohv",
    "outputId": "76b1c6b8-5868-4008-b6f4-b468117dae2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 20])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_l.get_prime().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CifUve92TEZ-"
   },
   "source": [
    "## Build sigmoid layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IUmoei9uTE1A"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self, **kw):\n",
    "        super().__init__(**kw)\n",
    "        self.__t = torch.tensor(1.)\n",
    "        self.set_is_derivable(True)\n",
    "\n",
    "    def backward(self, delta, features):\n",
    "        self.set_grads(None)\n",
    "\n",
    "    def get_prime(self, features):\n",
    "        # Suppose softmax over channel dim\n",
    "        x = self(features)\n",
    "        diff = torch.sub(self.__t, x, alpha=1, out=None)\n",
    "        x = torch.mul(x, diff, out=x)\n",
    "        return x\n",
    "\n",
    "    def get_weights(self):\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = torch.exp(-x, out=x)\n",
    "        x = torch.add(x, self.__t, alpha=1, out=x)\n",
    "        x = torch.div(self.__t, x, out=x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QT0wNGq1udy"
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "JH_jOksz4Yd8"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "class Model(object):\n",
    "    def __init__(self, device):\n",
    "        \"\"\"\"\"\"\n",
    "        self.__device = device\n",
    "        self.__layers = []\n",
    "        self.__parameteres  = []\n",
    "        self.__out_features = []\n",
    "        self.__metrics_fn = None\n",
    "\n",
    "    def serialization(self, layers: list):\n",
    "        if (isinstance(layers, list)):\n",
    "            for i, layer in enumerate(layers, 0):\n",
    "                if (not isinstance(layer, Layer)):\n",
    "                    raise NameError(\"The layer: '{}' is not as 'Layer' object\".format(i))\n",
    "            self.__layers = layers\n",
    "        else:\n",
    "            raise NameError(\"The argument need to be as 'List' object, but is '{}'\".format(type(layers)))\n",
    "\n",
    "    def compile(self, optimizer, loss, metrics, callbacks):\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss\n",
    "        self.metric_fn = MetricsList(metrics)\n",
    "        self.callbacks = CallbacksList(callbacks)\n",
    "\n",
    "    def backward(self, loss):\n",
    "        layer = self.__layers[-1]\n",
    "        delta = loss\n",
    "        #print(\"layer {}, shape 'delta' {}\".format(layer.name, delta.shape))\n",
    "        if (layer.is_derivable()):\n",
    "            prime_fn = self.__layers[-1].get_prime\n",
    "            #print(\"layer {}, '{}'\".format(layer.name, \"derivable\"))\n",
    "        else:\n",
    "            prime_fn = None\n",
    "            # output of the second-last layer\n",
    "            prev_features = self.__out_features[-2]\n",
    "            features = self.__out_features[-1]\n",
    "            # calculate the gradient for the cost function C_x\n",
    "            delta = torch.empty(features.size(), dtype=torch.float32)\n",
    "            delta = torch.add(delta, loss, alpha=1, out=delta)\n",
    "            layer.backward(delta, prev_features)\n",
    "            #print(\"layer {}, prev_layer '{}', D*Ft\".format(layer.name, self.__layers[-2].name))\n",
    "\n",
    "        #print(\"+++++++++++++++++++++++++++++++++++++++++\")\n",
    "        # backprop\n",
    "        for l in range(2, len(self.__layers), 1):\n",
    "            layer = self.__layers[-l]\n",
    "            #print(\"layer {}, shape: 'delta' {}\".format(layer.name, delta.shape))\n",
    "            # -----------\n",
    "            weights = self.__layers[-l+1].get_weights()\n",
    "            if (weights is not None):\n",
    "                #print(\"\\tnext_layer {}, shape: 'weights' {}, 'delta' {}, D*Wt\".format(self.__layers[-l+1].name, weights.shape, delta.shape))\n",
    "                weights = weights.transpose(1, 0).unsqueeze(0)\n",
    "                delta   = torch.matmul(weights, delta)\n",
    "                #print(\"'weights' -l+1, shape {}\".format(weights.shape))\n",
    "                #print(\" 'delta' -l, shape {}\".format(delta.shape))\n",
    "            else:\n",
    "                #print(\"not 'weights' -l+1\")\n",
    "                #print(\" 'delta' -l, shape {}\".format(delta.shape))\n",
    "                #print(\"\\tnext_layer {} not 'weights', \".format(self.__layers[-l+1].name))\n",
    "                pass\n",
    "\n",
    "            # =========\n",
    "            if (prime_fn is not None):\n",
    "                # Derivative of the last layer\n",
    "                # calculate gradients for layer -l\n",
    "                features = self.__out_features[-l]\n",
    "                prime = prime_fn(features)\n",
    "                #print(\"\\tnext_layer {}, shape: 'prime' {}, 'delta' {}, D*P\".format(self.__layers[-l+1].name, prime.shape, delta.shape))\n",
    "                delta = torch.mul(delta, prime)\n",
    "                #print(\"'prime' -l+1, shape {}\".format(prime.shape))\n",
    "                #print(\" 'delta' -l, shape {}\".format(delta.shape))\n",
    "            else:\n",
    "                #print(\"not 'prime' -l+1\")\n",
    "                #print(\" 'delta' -l, shape {}\".format(delta.shape))\n",
    "                #print(\"\\tnext_layer {} not 'prime', \".format(self.__layers[-l+1].name))\n",
    "                pass\n",
    "\n",
    "            if (layer.is_derivable()):\n",
    "                prime_fn = layer.get_prime\n",
    "                #print(\"'derivable' -l {}\".format(l))\n",
    "                #print(\"layer {} 'derivable'\".format(layer.name))\n",
    "            else:\n",
    "                prime_fn = None\n",
    "                # calculate gradients for layer -l\n",
    "                features = self.__out_features[-l-1]\n",
    "                #print(\"'not derivable' -l, shape {}\".format(features.shape))\n",
    "                #print(\" 'delta' -l, shape {}\".format(delta.shape))\n",
    "                layer.backward(delta, features)\n",
    "                #print(\"layer {}, prev_layer {}, not 'derivable', D*Ft\".format(layer.name, self.__layers[-l-1].name))\n",
    "        #print(\"+++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    def get_grads(self):\n",
    "        grads = []\n",
    "        for layer in self.__layers:\n",
    "            grads.append(layer.get_grads())\n",
    "        return grads\n",
    "\n",
    "    def parameters(self):\n",
    "        parameteres = []\n",
    "        for layer in self.__layers:\n",
    "            parameteres.append(layer.parameters())\n",
    "        return parameteres\n",
    "\n",
    "    def forward(self, train_ds, test_ds=None, epochs=1):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        if (not isinstance(train_ds, DataLoader)):\n",
    "            raise NameError(\"The train dataset: is type '{}' not as 'DataLoader' object\".format(type(train_ds)))\n",
    "        # start training\n",
    "        self.callbacks.on_train_begin()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_logs = self.train_batch(epoch, train_ds)\n",
    "        self.callbacks.on_train_end(epoch_logs)\n",
    "        return None\n",
    "\n",
    "    def train_batch(self, epoch, train_ds):\n",
    "        self.callbacks.on_epoch_begin(epoch)\n",
    "        batch_logs = {}\n",
    "        for i, data in zip(range(len(train_ds)), train_ds()):\n",
    "        #for i, data in zip(range(1), train_ds()):\n",
    "            self.callbacks.on_train_batch_begin(i)\n",
    "            batch_logs = self.train_step(data)\n",
    "            self.callbacks.on_train_batch_end(i, batch_logs)\n",
    "            # Empty cached memory\n",
    "            torch.cuda.empty_cache()\n",
    "        self.callbacks.on_epoch_end(epoch, batch_logs)\n",
    "\n",
    "    def train_step(self, data):\n",
    "        img, y = data\n",
    "        #print(\"type img {}\".format(type(img)))\n",
    "        img = img.to(self.__device)\n",
    "        y   = y.to(self.__device)\n",
    "        y_pred = self(img)\n",
    "        #print(\"y_pred {}, y {}\".format(y_pred.shape, y.shape))\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        #print(\"loss {}, data @{}@\".format(loss.shape, loss))\n",
    "        metrics = self.metric_fn(y_pred, y)\n",
    "        self.backward(loss)\n",
    "        grads = self.get_grads()\n",
    "        self.optimizer(grads)\n",
    "        loss = torch.mean(loss)\n",
    "        logs = {\"loss\":loss}\n",
    "        logs.update(metrics)\n",
    "        return logs\n",
    "\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        y = inputs\n",
    "        #print(\"model: shape {}\".format(y.shape))\n",
    "        for layer in self.__layers:\n",
    "            y = layer(y)\n",
    "            #print(\"model: name {}, shape {}\".format(layer.name, y.shape))\n",
    "            self.__out_features.append(y)\n",
    "        return y\n",
    "\n",
    "    def copy(self):\n",
    "        parameteres = []\n",
    "        for layer in self.__layers:\n",
    "            parameteres.append(layer.copy())\n",
    "        return parameteres\n",
    "\n",
    "    def cmp(self, m_parameters):\n",
    "        is_equal = True\n",
    "        for layer, l_parameters in zip(self.__layers, m_parameters):\n",
    "            tmp_equal = layer.cmp(l_parameters)\n",
    "            print(\"layer {}, cmp {}\".format(layer.name, tmp_equal))\n",
    "            is_equal = is_equal and tmp_equal\n",
    "        return is_equal\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives partial C_x\n",
    "        partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHI_6IyE4Yay",
    "outputId": "43517622-652c-48a2-e799-014a03f271e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "          1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "tmp_x = torch.ones(2, 3, 20)  # Uniform [-1, 1]\n",
    "print(tmp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v2JcAsVA4YXS",
    "outputId": "e25cf740-dd2f-4f39-ecca-b7ee90e2ae7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_x.transpose(2, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXc4vD9GdCvn"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "J3XwtU2uc8so"
   },
   "outputs": [],
   "source": [
    "# Pipiline\n",
    "train_ds = ExtendedMNISTDataset(root=\"/content\", train=True)\n",
    "test_ds = ExtendedMNISTDataset(root=\"/content\", train=False)\n",
    "# Data loader\n",
    "train_loader = DataLoader(train_ds, batchsize=32, shuffle=True)\n",
    "train_loader.map(lambda x, y: (normalization_n1_p1(x), one_hot(y, num_classes=10)))\n",
    "train_loader.map(lambda x, y: (x.unsqueeze(-1), y.unsqueeze(-1)))\n",
    "\n",
    "test_loader  = DataLoader(test_ds,  batchsize=20, shuffle=False)\n",
    "test_loader.map(lambda x, y: (normalization_n1_p1(x), one_hot(y, num_classes=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Qq0Ft24dTZ9",
    "outputId": "d91b7252-d466-41d8-a1e1-da2dd0dfef90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.accelerator.is_available():\n",
    "    device=torch.accelerator.current_accelerator()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6V07jRYzombR"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BjvLDYmfCmKK"
   },
   "outputs": [],
   "source": [
    "def pyTorch_init(shape):\n",
    "    lim = np.sqrt(1./shape[0])\n",
    "    return torch.empty(*shape).uniform_(-lim, lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxgOFnw6C-8Z",
    "outputId": "f79da63c-d712-41ac-a09b-47e86c4b59a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0898e-02, -7.3493e-05, -2.7475e-02,  ...,  1.7356e-02,\n",
       "         -8.7690e-03,  7.1056e-03],\n",
       "        [-2.4226e-02, -7.5569e-03, -2.6560e-02,  ...,  1.6351e-02,\n",
       "         -9.5651e-03,  3.0135e-02],\n",
       "        [-1.1401e-02,  1.2495e-02, -2.3525e-02,  ..., -7.4247e-03,\n",
       "         -3.1869e-02, -1.6953e-02],\n",
       "        ...,\n",
       "        [-4.0899e-03, -2.0507e-02,  2.1776e-02,  ..., -7.1401e-03,\n",
       "         -1.6032e-02, -1.4364e-02],\n",
       "        [-4.9581e-03, -1.9841e-02,  1.8665e-02,  ..., -3.1068e-02,\n",
       "          5.9023e-03, -1.0430e-02],\n",
       "        [ 3.2758e-02,  2.0944e-03, -1.3189e-03,  ..., -3.2962e-02,\n",
       "         -1.8022e-02,  2.1517e-02]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyTorch_init((784, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "2Rl5P8u8ex62"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model(device=device)\n",
    "layers = [\n",
    "    InputLayer(name=\"Inputs\"),\n",
    "    Dense(784, 100, init_fn=pyTorch_init, use_bias=True, init_fn_b=pyTorch_init, name=\"Dense_h1\"),\n",
    "    Relu(name=\"Relu_h0\"),\n",
    "    Dense(100, 10, init_fn=pyTorch_init, use_bias=True, init_fn_b=pyTorch_init, name=\"Dense_h2\"),\n",
    "    #Relu(name=\"Relu_h1\"),\n",
    "    #Dense(200, 100, init_fn=None, use_bias=False, init_fn_b=None, name=\"Dense_h2\"),\n",
    "    #Relu(name=\"Relu_h2\"),\n",
    "    #Dense(100, 10, init_fn=None, use_bias=False, init_fn_b=None, name=\"Dense_h3\"),\n",
    "    #Softmax(name=\"Softmax_out\"),\n",
    "    Sigmoid(name=\"sigmoid\"),\n",
    "]\n",
    "model.serialization(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "eGJDuTWu59nk"
   },
   "outputs": [],
   "source": [
    "c_model = model.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHxve_ij69D9",
    "outputId": "670687ca-b4d2-4e44-9cef-d6c39935a2d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer Inputs, size: loc 0, par 0\n",
      "layer Inputs, cmp True\n",
      "layer Dense_h1, size: loc 2, par 2\n",
      "layer Dense_h1, cmp True\n",
      "layer Relu_h0, size: loc 0, par 0\n",
      "layer Relu_h0, cmp True\n",
      "layer Dense_h2, size: loc 2, par 2\n",
      "layer Dense_h2, cmp True\n",
      "layer sigmoid, size: loc 0, par 0\n",
      "layer sigmoid, cmp True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cmp(c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Vadvn2V7Gto2"
   },
   "outputs": [],
   "source": [
    "def cost_derivative(output_activations, y):\n",
    "    \"\"\"Return the vector of partial derivatives partial C_x\n",
    "    partial a for the output activations.\"\"\"\n",
    "    return (output_activations-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3gNj4zczfbHl"
   },
   "outputs": [],
   "source": [
    "\n",
    "cros_entropy = torch.nn.CrossEntropyLoss()\n",
    "#cros_entropy = F.cross_entropy\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "plot_losses = PlotLossesKeras()\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=cros_entropy,\n",
    "    metrics={\"acuracy\":Acuracy(name=\"acuracy\")},\n",
    "    callbacks=[plot_losses\n",
    "               ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "id": "-Ul0nrBehP2Y",
    "outputId": "e3daea48-4de8-4e71-8ceb-1be7ba968750"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuracy\n",
      "\tacuracy          \t (min:    0.031, max:    0.031, cur:    0.031)\n",
      "Loss\n",
      "\tloss             \t (min:    2.303, max:    2.303, cur:      nan)\n"
     ]
    }
   ],
   "source": [
    "model.forward(train_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJupY-AF91To",
    "outputId": "b75528f5-2823-4c91-c24c-8ca2a2b30bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer Inputs, size: loc 0, par 0\n",
      "layer Inputs, cmp True\n",
      "layer Dense_h1, size: loc 1, par 1\n",
      "layer Dense_h1, cmp False\n",
      "layer Dense_h2, size: loc 1, par 1\n",
      "layer Dense_h2, cmp False\n",
      "layer Softmax_out, size: loc 0, par 0\n",
      "layer Softmax_out, cmp True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cmp(c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "jd7ReYbLaYIe",
    "outputId": "3a131002-663d-4c81-858d-557e1a25c9dc"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2528488726.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.get_grads()[3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "RODUjAzU8kEd"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syvi7Wap8okj",
    "outputId": "91852718-5d1b-42a7-e18b-329395ca5c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01766504 0.1177981  0.00631797 0.26399485 0.87546667 0.88055606\n",
      " 0.29744105 0.63327045 0.48407139 0.33574919]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(10)\n",
    "delta = np.random.uniform(low=0, high=1, size=(10))\n",
    "print(delta)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ur7x4QJ18sMo",
    "outputId": "becbfdfe-f495-456d-8fcd-cb4e346ee191"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.9123307723796983)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(delta, a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "x55A6JvfHaAK",
    "4xQA0TbS9iJz",
    "-gsQ54N4Hy2Y",
    "Yp35IrNCH1ED",
    "B6JV4BsOHsBz",
    "-9nd7JEAH2aC",
    "G8aGDKmc9tzF",
    "UVslqY8VHz_V",
    "AW3g5UadSVzk",
    "YGAwM2WMSaxa",
    "o5ucXnc8H2l_",
    "pdk7L7ObSgPJ",
    "rC-Fusm36iXQ",
    "kCNgANQO5sAu",
    "STQxIDrfjGhm",
    "kkCJdVkn1vwi",
    "hUCepVut2W9u",
    "-OVh9YZo02Fl",
    "CifUve92TEZ-"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
