{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNL7l79tOg8N"
   },
   "source": [
    "# A complex yet simple efficient training pipeline for CIFAR-10\n",
    "\n",
    "This pipeline serves an educational purpose, hence that's why it's complex yet simple.\n",
    "\n",
    "For a more complex and more efficient training pipeline for CIFAR-10, do check [CIFAR-10 speedruns: 94% in 2.6 seconds and 96% in 27 seconds](https://github.com/KellerJordan/cifar10-airbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "o3q_jy9POg8P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import freeze_support\n",
    "import time\n",
    "from timed_decorator.simple_timed import timed\n",
    "\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root path:\t/home/gheorghe/Desktop/Proiecte/master/CARN\n",
      "local path:\t/home/gheorghe/Desktop/Proiecte/master/CARN/laborator_2\n",
      "deep learning path:\t/home/gheorghe/Desktop/Proiecte/master/CARN/deep_learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LCL_PATH  = str(Path().cwd())\n",
    "ROOT_PATH = str(Path(LCL_PATH).parent)\n",
    "DEEPL_PATH = str(Path(ROOT_PATH)/\"deep_learning\")\n",
    "print(\"\"\"\n",
    "root path:\\t{}\n",
    "local path:\\t{}\n",
    "deep learning path:\\t{}\"\"\".format(ROOT_PATH, LCL_PATH, DEEPL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding local_folder to the system path\n",
    "sys.path.append(ROOT_PATH)\n",
    "sys.path.append(LCL_PATH)\n",
    "sys.path.append(DEEPL_PATH)\n",
    "\n",
    "from sys_function import * # este in root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_remove_modules(\"dataset.dataset_rand_append_unsupervised\")\n",
    "sys_remove_modules(\"trainer.unsupervised_trainer\")\n",
    "sys_remove_modules(\"my_transformers.one_hot\")\n",
    "sys_remove_modules(\"my_transformers.label_smoothing\")\n",
    "sys_remove_modules(\"models.unsupervised.resnet_unsupervised\")\n",
    "sys_remove_modules(\"conf_manager.train_conf\")\n",
    "sys_remove_modules(\"checks.tensor_check\")\n",
    "\n",
    "from dataset.dataset_rand_append_unsupervised import *\n",
    "from trainer.unsupervised_trainer import *\n",
    "from my_transformers.one_hot import *\n",
    "from my_transformers.label_smoothing import *\n",
    "from models.unsupervised.resnet_unsupervised import *\n",
    "from conf_manager.train_conf import *\n",
    "from checks.tensor_check import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dovlmJqOg8Q"
   },
   "source": [
    "First we define some configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IVDFRFM0Og8R"
   },
   "outputs": [],
   "source": [
    "disable_compile = True\n",
    "compile_is_slower = False\n",
    "BATCH_SIZE = 24\n",
    "IMAGE_SIZE = 32*2\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation/preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_transforms(transform, inputs, shape, max_val, min_val, dtype):\n",
    "    # check 1000 tests\n",
    "    for _ in range(1000):\n",
    "        x = transform(inputs)\n",
    "        tensor_check(x, shape, max_val, min_val, dtype)\n",
    "    else:\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transforms(image_size: int):\n",
    "    # These transformations are cached.\n",
    "    # We could have used RandomCrop with padding. But we are smart, and we know we cache the initial_transforms so\n",
    "    # we don't compute them during runtime. Therefore, we do the padding beforehand, and apply cropping only at\n",
    "    # runtime\n",
    "    random_choice = v2.RandomChoice([\n",
    "        v2.RandomPerspective(\n",
    "                    distortion_scale=0.15, # controls how much each corner can move. \n",
    "                    p=1.0),                # probability of applying the effect\n",
    "        v2.RandomRotation(degrees=30),     # rotates an image with random angle\n",
    "        v2.RandomAffine(\n",
    "                    degrees=30,             # rotation ±30\n",
    "                    translate=(0.15, 0.15), # horizontal/vertical translation as fraction of image\n",
    "                    scale=(0.75, 1.05),     # scale factor\n",
    "                    shear=10),              # shear angle ±10°\n",
    "        v2.RandomCrop(\n",
    "                    size=image_size,   # height & width of crop\n",
    "                    padding=4),        # pixels to pad around the image\n",
    "        v2.RandomResizedCrop(\n",
    "                    size=image_size,\n",
    "                    scale=(0.75, 1.),  # range of area proportion to crop from the original image\n",
    "                    ratio=(0.8,  1.)), # range of aspect ratio (width/height)\n",
    "        v2.RandomAdjustSharpness(\n",
    "                    sharpness_factor=1.5, # controls the degree of sharpness; ( >1 sharpened; <1 slightly blurred)\n",
    "                    p=1.),                      # probability of applying the transform\n",
    "        v2.RandomAutocontrast(p=1.), # probability of applying the transform\n",
    "        v2.RandomEqualize( # histogram of pixel values\n",
    "                    p=1.), # probability of applying the transform\n",
    "        v2.ColorJitter(  # randomly changes the brightness, contrast, saturation, and hue\n",
    "                    brightness=0.5, # factor to change brightness\n",
    "                    contrast=0.3,   # factor to change contrast\n",
    "                    saturation=0.3, # factor to change saturation\n",
    "                    hue=0.3,),      # factor to change hue\n",
    "        v2.GaussianBlur(  # applies a Gaussian blur\n",
    "                    kernel_size=(7, 7), # size of the Gaussian kernel\n",
    "                    # standard deviation of the Gaussian kernel; a float or tuple (min, max) for random sampling\n",
    "                    sigma=(0.1, 5.)),   # how to handle image borders\n",
    "        v2.RandomErasing(\n",
    "                    scale=(0.01, 0.15), # range of area ratio to erase (relative to image area)\n",
    "                    value=10,           # fill value: single number, tuple, or 'random'\n",
    "                    inplace=False,      # whether to erase in place or return a new image\n",
    "                    p=1.),              # probability of applying the transform\n",
    "        v2.Grayscale(num_output_channels=3), # number of channels in output image: 1 or 3\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.Identity(),  # returns the input image unchanged\n",
    "    ])\n",
    "    transforms = v2.Compose([\n",
    "        # if use 'ToImage' tensor should be numpy array!!!\n",
    "        v2.ToImage(), # data are transorm to torch tensor in Dataset manager, tensor should be numpy array!!!\n",
    "        v2.Resize(\n",
    "            size=int(image_size),),\n",
    "        v2.CenterCrop(image_size),\n",
    "        random_choice,\n",
    "        v2.ToDtype(torch.float32, scale=True), # converts uint8 [0,255] -> float32 [0,1]\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                     std=(0.5, 0.5, 0.5), \n",
    "                     inplace=True),\n",
    "        ])\n",
    "    # We use the inplace flag because we can safely change the tensors inplace when normalize is used.\n",
    "    # For is_train=False, we can safely change the tensors inplace because we do it only once, when caching.\n",
    "    # For is_train=True, we can safely change the tensors inplace because we clone the cached tensors first.\n",
    "\n",
    "    # Q: How to make this faster?\n",
    "    # A: Use batched runtime transformations.\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = get_transforms(IMAGE_SIZE)\n",
    "inputs = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8) + 255\n",
    "shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "check_transforms(transform, inputs, shape, 1, -1, torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar10_train = CIFAR10(root=\"./data\", train=True,  transform=None, download=True)\n",
    "print(str(cifar10_train))\n",
    "cifar10_test  = CIFAR10(root=\"./data\", train=False, transform=None, download=True)\n",
    "print(str(cifar10_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_train = dict(inputs=np.array(cifar10_train.data, dtype=np.uint8), \n",
    "               targets=np.array(cifar10_train.targets, dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)\n",
    "d_test  = dict(inputs=np.array(cifar10_test.data,  dtype=np.uint8), \n",
    "               targets=np.array(cifar10_test.targets,  dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train[\"inputs\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6uR-23mOg8S",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Efficient in-memory dataset wrapper for caching\n",
    "\n",
    "Beware that this dataset keeps all data in memory. If it is too large, we might opt to cache the data on the disk and read it in `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = DatasetRandAppendUsupervised(d_train, transform=get_transforms(IMAGE_SIZE), \n",
    "                             train=True, freq_rand=10)\n",
    "test_ds  = DatasetRandAppendUsupervised(d_test,  transform=get_transforms(IMAGE_SIZE), \n",
    "                             train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape  = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = ()\n",
    "for idx in range(1000):\n",
    "    inputs = train_ds[idx]\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32, arr_type=torchvision.tv_tensors._image.Image)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@timed(use_seconds=True, show_args=True, return_time=True)\n",
    "def load_data(dataset, num_workers: int):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "    for _ in dataloader:\n",
    "        pass  # Simulate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data(DatasetRandAppendUsupervised[10000], 0) -> total time: 2.777121876s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 1) -> total time: 3.006498065s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 2) -> total time: 1.528017876s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 3) -> total time: 1.074329471s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 4) -> total time: 0.825065492s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 5) -> total time: 0.755389131s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 6) -> total time: 0.690756489s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 7) -> total time: 0.809510092s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 8) -> total time: 0.754513457s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 9) -> total time: 0.768643727s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 10) -> total time: 0.759578060s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 11) -> total time: 0.773990491s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 12) -> total time: 0.698934928s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 13) -> total time: 0.684469652s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 14) -> total time: 0.638338888s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 15) -> total time: 0.654311095s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 16) -> total time: 0.634430437s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 17) -> total time: 0.607811019s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 18) -> total time: 0.601835567s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 19) -> total time: 0.639433362s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 20) -> total time: 0.634398254s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 21) -> total time: 0.598887479s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 22) -> total time: 0.595226454s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 23) -> total time: 0.592550366s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 24) -> total time: 0.596908442s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 25) -> total time: 0.593588456s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 26) -> total time: 0.588188401s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 27) -> total time: 0.602686861s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 28) -> total time: 0.620114416s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 29) -> total time: 0.669978554s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 30) -> total time: 0.666443914s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 31) -> total time: 0.643020922s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 32) -> total time: 0.635148136s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 33) -> total time: 0.686146874s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 34) -> total time: 0.667097506s\n",
      "load_data(DatasetRandAppendUsupervised[10000], 35) -> total time: 0.709903896s\n",
      "argmin 26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(0, 36):\n",
    "    _, t0 = load_data(test_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data(DatasetRandAppendUsupervised[50000], 0) -> total time: 16.227242009s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 1) -> total time: 14.988720305s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 2) -> total time: 7.607223795s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 3) -> total time: 5.193864064s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 4) -> total time: 4.148502362s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 5) -> total time: 3.666136183s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 6) -> total time: 3.619762722s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 7) -> total time: 3.832911870s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 8) -> total time: 3.659140553s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 9) -> total time: 3.542378216s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 10) -> total time: 3.246239622s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 11) -> total time: 3.050290037s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 12) -> total time: 2.943872441s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 13) -> total time: 2.752325047s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 14) -> total time: 2.585593349s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 15) -> total time: 2.469996194s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 16) -> total time: 2.429849414s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 17) -> total time: 2.298967974s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 18) -> total time: 2.215642986s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 19) -> total time: 2.158891315s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 20) -> total time: 2.069916007s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 21) -> total time: 1.971520468s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 22) -> total time: 1.955433364s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 23) -> total time: 1.927536612s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 24) -> total time: 1.853320910s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 25) -> total time: 1.817453062s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 26) -> total time: 1.722819893s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 27) -> total time: 1.701287092s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 28) -> total time: 1.669288182s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 29) -> total time: 1.699282498s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 30) -> total time: 1.669348832s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 31) -> total time: 1.649721868s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 32) -> total time: 1.689134692s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 33) -> total time: 1.679331864s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 34) -> total time: 1.727695343s\n",
      "load_data(DatasetRandAppendUsupervised[50000], 35) -> total time: 1.723199734s\n",
      "argmin 31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(0, 36):\n",
    "    _, t0 = load_data(train_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the best number workers\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=False, num_workers=26, drop_last=False)\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=31, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okXAgQ-DOg8T"
   },
   "source": [
    "This is the classification model, which leverages PyTorch Image Models to create backbones.\n",
    "\n",
    "Beware that not all backbones have a fully connected (fc) layer at the end. Some of them do, especially the resnet variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Input=(img_channels, out_channels, kernel_size, stride)\n",
    "body={body_name={in_channels, expansion, stride, intermediate_channels, num_residual_blocks}}\n",
    "Output=(in_features, out_features)\n",
    "\"\"\"\n",
    "resnet_conf = dict(\n",
    "    Input=dict(\n",
    "        img_channels=3, \n",
    "        out_channels=9, \n",
    "        kernel_size=3, \n",
    "        stride=1),\n",
    "    encode=dict(\n",
    "        enc_conv_1x=dict(in_channels=9, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=32, \n",
    "                     num_residual_blocks=5),\n",
    "        enc_conv_2x=dict(in_channels=128, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=64, \n",
    "                     num_residual_blocks=4),\n",
    "        enc_conv_3x=dict(in_channels=256, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=3),\n",
    "    ),\n",
    "    decode=dict(\n",
    "        dec_conv_1x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "        dec_conv_2x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "        dec_conv_3x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "    ),\n",
    "    Output=(1024, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNetUnsupervised(\"resnet_32x32_cifar\", **resnet_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetUnsupervised(\n",
       "  (input): InputStride(\n",
       "    (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1), groups=3, bias=False)\n",
       "    (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encode): IdentityResNetBlock(\n",
       "    (block): Sequential(\n",
       "      (enc_conv_1x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(9, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "        (identity_downsample): IdentityConv2dDownSample(\n",
       "          (conv1): Conv2d(9, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (enc_conv_1x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_1x_2): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_1x_3): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_1x_4): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_2x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "        (identity_downsample): IdentityConv2dDownSample(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (enc_conv_2x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_2x_2): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_2x_3): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_3x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "        (identity_downsample): IdentityConv2dDownSample(\n",
       "          (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (enc_conv_3x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (enc_conv_3x_2): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode): IdentityResNetUpBlock(\n",
       "    (block): Sequential(\n",
       "      (dec_conv_1x_0_upsample): Conv2dUpSample(\n",
       "        (conv1): Conv2d(1024, 4096, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dec_conv_1x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (dec_conv_1x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (dec_conv_2x_0_upsample): Conv2dUpSample(\n",
       "        (conv1): Conv2d(1024, 4096, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dec_conv_2x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (dec_conv_2x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (dec_conv_3x_0_upsample): Conv2dUpSample(\n",
       "        (conv1): Conv2d(1024, 4096, kernel_size=(3, 3), stride=(1, 1), padding=same, groups=1024, bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (dec_conv_3x_0): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "      (dec_conv_3x_1): IdentityResNetModule(\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (activ_fn): SiLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Conv2d(1024, 3, kernel_size=(1, 1), stride=(1, 1), padding=same, bias=False)\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "device    = torch.device(\"cuda\")\n",
    "trainer_obj = UnsupervisedTrainer(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            device=device,\n",
    "            type_compile=\"normal\",\n",
    "            disable_tqdm=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|▌| 55/100 [2:39:27<2:10:28, 173.96s/it, train_loss=1.43e-5, val_l\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCH \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      2\u001b[0m history_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(LCL_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs/unsupervised_deep_5x_conf_logs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhistory_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Proiecte/master/CARN/deep_learning/trainer/unsupervised_trainer.py:117\u001b[0m, in \u001b[0;36mUnsupervisedTrainer.run\u001b[0;34m(self, train_dl, val_dl, epochs, save_path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m--> 117\u001b[0m         tr_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m         va_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval(val_dl)\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (va_loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_va_loss):\n",
      "File \u001b[0;32m~/Desktop/Proiecte/master/CARN/deep_learning/trainer/unsupervised_trainer.py:77\u001b[0m, in \u001b[0;36mUnsupervisedTrainer.train\u001b[0;34m(self, train_ds)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This metric is actually an approximation of an accuracy, we are checking whether the dominant class\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# predicted by the model is also equal to the dominant soft label\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# The reason we are moving the data from device back to CPU is because these calculations are usually\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# faster on CPU for small batch sizes\u001b[39;00m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# We use detach because we tell the autograd engine to not track the gradients for predicted anymore\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     total      \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCH = 100\n",
    "history_path = \"{}/{}\".format(LCL_PATH, \"logs/unsupervised_deep_5x_conf_logs.pth\")\n",
    "trainer_obj.run(train_dl, test_dl, EPOCH, history_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOhQ2VIMOg8V"
   },
   "source": [
    "The comments are self-explainatory. If you do not know what a transformation does, the official documentation is your friend.\n",
    "Reading documentation helps your brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q47ymPCBOg8Z"
   },
   "source": [
    "The full training script is available in [complex_yet_simple_training_pipeline.py](./complex_yet_simple_training_pipeline.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r36R_vT5Og8Z"
   },
   "source": [
    "## Excercises\n",
    "\n",
    "1. Create your own efficient training pipeline for CIFAR-10.\n",
    "2. Adapt your pipeline (and this pipeline) to use some batched transformations. Measure the speedup!\n",
    "3. Adapt your pipeline (and this pipeline) to include Automatic Mixed Precision. Read the documentation first!\n",
    "4. Adjust your pipeline (or this pipeline) to achieve 96% on CIFAR-10 (hard). You may change the model, but pretrained weights are forbidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F31jwuDOg8Z"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVwcPWNsOg8a"
   },
   "source": [
    "| All     | [advanced_pytorch/](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch) |\n",
    "|---------|-- |\n",
    "| Current | [A complex yet simple efficient training pipeline for CIFAR-10](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
