{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNL7l79tOg8N"
   },
   "source": [
    "# A complex yet simple efficient training pipeline for CIFAR-10\n",
    "\n",
    "This pipeline serves an educational purpose, hence that's why it's complex yet simple.\n",
    "\n",
    "For a more complex and more efficient training pipeline for CIFAR-10, do check [CIFAR-10 speedruns: 94% in 2.6 seconds and 96% in 27 seconds](https://github.com/KellerJordan/cifar10-airbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o3q_jy9POg8P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import freeze_support\n",
    "import time\n",
    "from timed_decorator.simple_timed import timed\n",
    "\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale\n",
      "local path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/laborator_2\n",
      "deep learning path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LCL_PATH  = str(Path().cwd())\n",
    "ROOT_PATH = str(Path(LCL_PATH).parent)\n",
    "DEEPL_PATH = str(Path(ROOT_PATH)/\"deep_learning\")\n",
    "print(\"\"\"\n",
    "root path:\\t{}\n",
    "local path:\\t{}\n",
    "deep learning path:\\t{}\"\"\".format(ROOT_PATH, LCL_PATH, DEEPL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding local_folder to the system path\n",
    "sys.path.append(ROOT_PATH)\n",
    "sys.path.append(LCL_PATH)\n",
    "sys.path.append(DEEPL_PATH)\n",
    "\n",
    "from sys_function import * # este in root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_remove_modules(\"dataset.dataset_rand_append_unsupervised\")\n",
    "sys_remove_modules(\"trainer.unsupervised_trainer\")\n",
    "sys_remove_modules(\"transformers.one_hot\")\n",
    "sys_remove_modules(\"transformers.label_smoothing\")\n",
    "sys_remove_modules(\"models.unsupervised.resnet_unsupervised\")\n",
    "sys_remove_modules(\"conf_manager.train_conf\")\n",
    "sys_remove_modules(\"checks.tensor_check\")\n",
    "\n",
    "from dataset.dataset_rand_append_unsupervised import *\n",
    "from trainer.unsupervised_trainer import *\n",
    "from transformers.one_hot import *\n",
    "from transformers.label_smoothing import *\n",
    "from models.unsupervised.resnet_unsupervised import *\n",
    "from conf_manager.train_conf import *\n",
    "from checks.tensor_check import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dovlmJqOg8Q"
   },
   "source": [
    "First we define some configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IVDFRFM0Og8R"
   },
   "outputs": [],
   "source": [
    "disable_compile = True\n",
    "compile_is_slower = False\n",
    "BATCH_SIZE = 24\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation/preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_transforms(transform, inputs, shape, max_val, min_val, dtype):\n",
    "    # check 1000 tests\n",
    "    for _ in range(1000):\n",
    "        x = transform(inputs)\n",
    "        tensor_check(x, shape, max_val, min_val, dtype)\n",
    "    else:\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transforms(image_size: int):\n",
    "    # These transformations are cached.\n",
    "    # We could have used RandomCrop with padding. But we are smart, and we know we cache the initial_transforms so\n",
    "    # we don't compute them during runtime. Therefore, we do the padding beforehand, and apply cropping only at\n",
    "    # runtime\n",
    "    random_choice = v2.RandomChoice([\n",
    "        v2.RandomPerspective(\n",
    "                    distortion_scale=0.15, # controls how much each corner can move. \n",
    "                    p=1.0),                # probability of applying the effect\n",
    "        v2.RandomRotation(degrees=30),     # rotates an image with random angle\n",
    "        v2.RandomAffine(\n",
    "                    degrees=30,             # rotation ±30\n",
    "                    translate=(0.15, 0.15), # horizontal/vertical translation as fraction of image\n",
    "                    scale=(0.75, 1.05),     # scale factor\n",
    "                    shear=10),              # shear angle ±10°\n",
    "        v2.RandomCrop(\n",
    "                    size=image_size,   # height & width of crop\n",
    "                    padding=4),        # pixels to pad around the image\n",
    "        v2.RandomResizedCrop(\n",
    "                    size=image_size,\n",
    "                    scale=(0.75, 1.),  # range of area proportion to crop from the original image\n",
    "                    ratio=(0.8,  1.)), # range of aspect ratio (width/height)\n",
    "        v2.RandomAdjustSharpness(\n",
    "                    sharpness_factor=1.5, # controls the degree of sharpness; ( >1 sharpened; <1 slightly blurred)\n",
    "                    p=1.),                      # probability of applying the transform\n",
    "        v2.RandomAutocontrast(p=1.), # probability of applying the transform\n",
    "        v2.RandomEqualize( # histogram of pixel values\n",
    "                    p=1.), # probability of applying the transform\n",
    "        v2.ColorJitter(  # randomly changes the brightness, contrast, saturation, and hue\n",
    "                    brightness=0.5, # factor to change brightness\n",
    "                    contrast=0.3,   # factor to change contrast\n",
    "                    saturation=0.3, # factor to change saturation\n",
    "                    hue=0.3,),      # factor to change hue\n",
    "        v2.GaussianBlur(  # applies a Gaussian blur\n",
    "                    kernel_size=(7, 7), # size of the Gaussian kernel\n",
    "                    # standard deviation of the Gaussian kernel; a float or tuple (min, max) for random sampling\n",
    "                    sigma=(0.1, 5.)),   # how to handle image borders\n",
    "        v2.RandomErasing(\n",
    "                    scale=(0.01, 0.15), # range of area ratio to erase (relative to image area)\n",
    "                    value=10,           # fill value: single number, tuple, or 'random'\n",
    "                    inplace=False,      # whether to erase in place or return a new image\n",
    "                    p=1.),              # probability of applying the transform\n",
    "        v2.Grayscale(num_output_channels=3), # number of channels in output image: 1 or 3\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.Identity(),  # returns the input image unchanged\n",
    "    ])\n",
    "    transforms = v2.Compose([\n",
    "        # if use 'ToImage' tensor should be numpy array!!!\n",
    "        v2.ToImage(), # data are transorm to torch tensor in Dataset manager, tensor should be numpy array!!!\n",
    "        v2.Resize(\n",
    "            size=int(image_size*1.3),),\n",
    "        v2.CenterCrop(image_size),\n",
    "        random_choice,\n",
    "        v2.ToDtype(torch.float32, scale=True), # converts uint8 [0,255] -> float32 [0,1]\n",
    "        v2.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                     std=(0.5, 0.5, 0.5), \n",
    "                     inplace=True),\n",
    "        ])\n",
    "    # We use the inplace flag because we can safely change the tensors inplace when normalize is used.\n",
    "    # For is_train=False, we can safely change the tensors inplace because we do it only once, when caching.\n",
    "    # For is_train=True, we can safely change the tensors inplace because we clone the cached tensors first.\n",
    "\n",
    "    # Q: How to make this faster?\n",
    "    # A: Use batched runtime transformations.\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = get_transforms(IMAGE_SIZE)\n",
    "inputs = np.zeros((IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.uint8) + 255\n",
    "shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "check_transforms(transform, inputs, shape, 1, -1, torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar10_train = CIFAR10(root=\"./data\", train=True, transform=None, download=True)\n",
    "print(str(cifar10_train))\n",
    "cifar10_test = CIFAR10(root=\"./data\", train=False, transform=None, download=True)\n",
    "print(str(cifar10_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_train = dict(inputs=np.array(cifar10_train.data, dtype=np.uint8), \n",
    "               targets=np.array(cifar10_train.targets, dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)\n",
    "d_test  = dict(inputs=np.array(cifar10_test.data,  dtype=np.uint8), \n",
    "               targets=np.array(cifar10_test.targets,  dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train[\"inputs\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6uR-23mOg8S",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Efficient in-memory dataset wrapper for caching\n",
    "\n",
    "Beware that this dataset keeps all data in memory. If it is too large, we might opt to cache the data on the disk and read it in `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = DatasetRandAppendUsupervised(d_train, transform=get_transforms(IMAGE_SIZE), \n",
    "                             train=True, freq_rand=10)\n",
    "test_ds  = DatasetRandAppendUsupervised(d_test,  transform=get_transforms(IMAGE_SIZE), \n",
    "                             train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape  = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = ()\n",
    "for idx in range(1000):\n",
    "    inputs = train_ds[idx]\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32, arr_type=torchvision.tv_tensors._image.Image)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@timed(use_seconds=True, show_args=True, return_time=True)\n",
    "def load_data(dataset, num_workers: int):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "    for _ in dataloader:\n",
    "        pass  # Simulate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(32, 36):\n",
    "    _, t0 = load_data(test_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(32, 36):\n",
    "    _, t0 = load_data(train_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the best number workers\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=30, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=False, num_workers=30, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "for idx, inputs in zip(range(1000), train_dl):\n",
    "    tensor_check(inputs, in_shape, 1, -1, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okXAgQ-DOg8T"
   },
   "source": [
    "This is the classification model, which leverages PyTorch Image Models to create backbones.\n",
    "\n",
    "Beware that not all backbones have a fully connected (fc) layer at the end. Some of them do, especially the resnet variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input=(img_channels, out_channels, kernel_size, stride)\n",
    "body={body_name={in_channels, expansion, stride, intermediate_channels, num_residual_blocks}}\n",
    "Output=(in_features, out_features)\n",
    "\"\"\"\n",
    "resnet_conf = dict(\n",
    "    Input=dict(\n",
    "        img_channels=3, \n",
    "        out_channels=9, \n",
    "        kernel_size=3, \n",
    "        stride=1),\n",
    "    encode=dict(\n",
    "        enc_conv_1x=dict(in_channels=9, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=32, \n",
    "                     num_residual_blocks=3),\n",
    "        enc_conv_2x=dict(in_channels=128, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=64, \n",
    "                     num_residual_blocks=4),\n",
    "        enc_conv_3x=dict(in_channels=256, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=3),\n",
    "    ),\n",
    "    decode=dict(\n",
    "        dec_conv_1x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "        dec_conv_2x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "        dec_conv_3x=dict(in_channels=1024, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=2),\n",
    "    ),\n",
    "    Output=(1024, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = ResNetUnsupervised(\"resnet_32x32_cifar\", **resnet_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gheorghe/venvs/torch_env/lib/python3.12/site-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "trainer_obj = UnsupervisedTrainer(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            use_cpu=False,\n",
    "            type_compile=\"normal\",\n",
    "            disable_tqdm=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_epoch None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCH = 100\n",
    "history_path = \"{}/{}\".format(LCL_PATH, \"logs/unsupervised_deep_5x_conf_logs.csv\")\n",
    "run_conf_obj = RunConfigs(model, trainer_obj, epochs=EPOCH, train_dl=train_dl, val_dl=test_dl, history_path=history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adam': {'optimizer': torch.optim.adam.Adam,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_momentum_nesterov': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_momentum_nesterov_weight_decay': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9,\n",
       "   'nesterov': True,\n",
       "   'weight_decay': 0.01,\n",
       "   'fused': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'adamW': {'optimizer': torch.optim.adamw.AdamW,\n",
       "  'opt_hyperparameters': {'fused': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_scheduler': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}},\n",
       " 'sgd_momentum_nesterov_scheduler': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}},\n",
       " 'adam_scheduler': {'optimizer': torch.optim.adam.Adam,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# name:(optimizer, (lr_scheduler=..., opt_hyperparameters=..., lr=float))\n",
    "\n",
    "multiple_runs_conf = dict(\n",
    "    adam=dict(optimizer=torch.optim.Adam,\n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd_momentum_nesterov=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd_momentum_nesterov_weight_decay=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True, weight_decay=0.01, fused=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    adamW=dict(optimizer=torch.optim.AdamW,\n",
    "            opt_hyperparameters=dict(fused=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    # scheduler\n",
    "    sgd_scheduler=dict(optimizer=torch.optim.SGD,\n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    "    sgd_momentum_nesterov_scheduler=dict(optimizer=torch.optim.SGD,\n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    "    adam_scheduler=dict(optimizer=torch.optim.Adam, \n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    ")\n",
    "multiple_runs_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'adam', conf {'optimizer': <class 'torch.optim.adam.Adam'>, 'opt_hyperparameters': None, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [4:47:12<00:00, 172.33s/it, train_loss=4.96e-5, val_loss=5.53e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'sgd', conf {'optimizer': <class 'torch.optim.sgd.SGD'>, 'opt_hyperparameters': None, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██████████████████████████████████████████▋                                                                                                                   | 27/100 [1:17:50<3:30:26, 172.97s/it, train_loss=0.0025, val_loss=0.00192]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_conf_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmultiple_runs_conf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/conf_manager/train_conf.py:56\u001b[39m, in \u001b[36mRunConfigs.__call__\u001b[39m\u001b[34m(self, **confs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     55\u001b[39m     conf = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/conf_manager/train_conf.py:45\u001b[39m, in \u001b[36mRunConfigs.run\u001b[39m\u001b[34m(self, name, **conf)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# antreneaza modelul\u001b[39;00m\n\u001b[32m     44\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m logs[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] = name\n\u001b[32m     47\u001b[39m logs[\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m] = time.time()-start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/trainer/unsupervised_trainer.py:118\u001b[39m, in \u001b[36mUnsupervisedTrainer.run\u001b[39m\u001b[34m(self, train_dl, val_dl, epochs, save_path)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m    117\u001b[39m     tr_loss = \u001b[38;5;28mself\u001b[39m.train(train_dl)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     va_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (va_loss < \u001b[38;5;28mself\u001b[39m.best_va_loss):\n\u001b[32m    120\u001b[39m         \u001b[38;5;28mself\u001b[39m.best_va_loss = va_loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_env/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/trainer/unsupervised_trainer.py:104\u001b[39m, in \u001b[36mUnsupervisedTrainer.val\u001b[39m\u001b[34m(self, val_ds)\u001b[39m\n\u001b[32m    101\u001b[39m data = data.to(\u001b[38;5;28mself\u001b[39m.device, non_blocking=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    103\u001b[39m predicted = \u001b[38;5;28mself\u001b[39m.model(data)\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Here we don't need to argmax the target, because we have hard labels. We don't use DA during validation.\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# We don't need to detach, because we are already in inference_mode\u001b[39;00m\n\u001b[32m    108\u001b[39m total      += data.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_conf_obj(**multiple_runs_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOhQ2VIMOg8V"
   },
   "source": [
    "The comments are self-explainatory. If you do not know what a transformation does, the official documentation is your friend.\n",
    "Reading documentation helps your brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q47ymPCBOg8Z"
   },
   "source": [
    "The full training script is available in [complex_yet_simple_training_pipeline.py](./complex_yet_simple_training_pipeline.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r36R_vT5Og8Z"
   },
   "source": [
    "## Excercises\n",
    "\n",
    "1. Create your own efficient training pipeline for CIFAR-10.\n",
    "2. Adapt your pipeline (and this pipeline) to use some batched transformations. Measure the speedup!\n",
    "3. Adapt your pipeline (and this pipeline) to include Automatic Mixed Precision. Read the documentation first!\n",
    "4. Adjust your pipeline (or this pipeline) to achieve 96% on CIFAR-10 (hard). You may change the model, but pretrained weights are forbidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F31jwuDOg8Z"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVwcPWNsOg8a"
   },
   "source": [
    "| All     | [advanced_pytorch/](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch) |\n",
    "|---------|-- |\n",
    "| Current | [A complex yet simple efficient training pipeline for CIFAR-10](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
