{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNL7l79tOg8N"
   },
   "source": [
    "# A complex yet simple efficient training pipeline for CIFAR-10\n",
    "\n",
    "This pipeline serves an educational purpose, hence that's why it's complex yet simple.\n",
    "\n",
    "For a more complex and more efficient training pipeline for CIFAR-10, do check [CIFAR-10 speedruns: 94% in 2.6 seconds and 96% in 27 seconds](https://github.com/KellerJordan/cifar10-airbench)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "o3q_jy9POg8P"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.transforms import v2\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from multiprocessing import freeze_support\n",
    "import time\n",
    "from timed_decorator.simple_timed import timed\n",
    "\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "root path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale\n",
      "local path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/laborator_2\n",
      "deep learning path:\t/home/gheorghe/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LCL_PATH  = str(Path().cwd())\n",
    "ROOT_PATH = str(Path(LCL_PATH).parent)\n",
    "DEEPL_PATH = str(Path(ROOT_PATH)/\"deep_learning\")\n",
    "print(\"\"\"\n",
    "root path:\\t{}\n",
    "local path:\\t{}\n",
    "deep learning path:\\t{}\"\"\".format(ROOT_PATH, LCL_PATH, DEEPL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# adding local_folder to the system path\n",
    "sys.path.append(ROOT_PATH)\n",
    "sys.path.append(LCL_PATH)\n",
    "sys.path.append(DEEPL_PATH)\n",
    "\n",
    "from sys_function import * # este in root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys_remove_modules(\"dataset.dataset_rand_append\")\n",
    "sys_remove_modules(\"trainer.trainer\")\n",
    "sys_remove_modules(\"initializers.xavier\")\n",
    "sys_remove_modules(\"transformers.one_hot\")\n",
    "sys_remove_modules(\"transformers.label_smoothing\")\n",
    "sys_remove_modules(\"models.supervised.resnet\")\n",
    "sys_remove_modules(\"conf_manager.train_conf\")\n",
    "\n",
    "from dataset.dataset_rand_append import *\n",
    "from trainer.trainer import *\n",
    "from initializers.xavier import *\n",
    "from transformers.one_hot import *\n",
    "from transformers.label_smoothing import *\n",
    "from models.supervised.resnet import *\n",
    "from conf_manager.train_conf import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dovlmJqOg8Q"
   },
   "source": [
    "First we define some configuration variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IVDFRFM0Og8R"
   },
   "outputs": [],
   "source": [
    "disable_compile = True\n",
    "compile_is_slower = False\n",
    "BATCH_SIZE = 24\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Manipulation/preprocesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Check function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_inputs(inputs, shape, max_val, min_val, dtype, arr_type=None):\n",
    "    # test max value\n",
    "    error_msg = \"Error: the max value should be less than '{}', is '{}'\".format(max_val, inputs.max())\n",
    "    assert (inputs.max() <= max_val), error_msg\n",
    "    # check min value\n",
    "    error_msg = \"Error: the min value should be bigger than '{}', is '{}'\".format(min_val, inputs.min())\n",
    "    assert (inputs.min() >= min_val), error_msg\n",
    "    # check dtype\n",
    "    error_msg = \"Error: type should be '{}', is '{}'\".format(dtype, inputs.dtype)\n",
    "    assert (inputs.dtype == dtype), error_msg\n",
    "    # check dtype\n",
    "    if (arr_type is not None):\n",
    "        error_msg = \"Error: input type should be '{}', is '{}'\".format(arr_type, type(inputs))\n",
    "        assert (isinstance(inputs, arr_type)), error_msg\n",
    "    #check shape\n",
    "    error_msg = \"Error: shape should be '{}', is '{}'\".format(shape, inputs.shape)\n",
    "    assert (inputs.shape == shape), error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_transforms(transform, inputs, shape, max_val, min_val, dtype):\n",
    "    # check 1000 tests\n",
    "    for _ in range(1000):\n",
    "        x = transform(inputs)\n",
    "        check_inputs(x, shape, max_val, min_val, dtype)\n",
    "    else:\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_all_transforms(transform, inputs, outputs, in_shape, out_shape, max_val, min_val, dtype):\n",
    "    for _ in range(1000):\n",
    "        x, y = transform(inputs, outputs)\n",
    "        check_inputs(x, in_shape, max_val, min_val, dtype)\n",
    "        check_inputs(y, out_shape, 1.5, -1.3, dtype)\n",
    "    else:\n",
    "        print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Init transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_init_transform(train, image_size):\n",
    "    transform = [ # image->tensor->resize->make square-> \n",
    "            # if use 'ToImage' tensor should be numpy array!!!\n",
    "            v2.ToImage(), # data are transorm to torch tensor in Dataset manager, tensor should be numpy array!!!\n",
    "            v2.Resize(\n",
    "                size=int(image_size*1.3),),\n",
    "            v2.CenterCrop(image_size),\n",
    "        ]\n",
    "    if (train == False):\n",
    "        transform.extend([\n",
    "                v2.ToDtype(torch.float32, scale=True),\n",
    "                v2.Normalize(mean=(0.491, 0.482, 0.446),  # need to scale normalization work with range 0...1\n",
    "                             std=(0.247, 0.243, 0.261), \n",
    "                             inplace=True),\n",
    "            ])\n",
    "                # We use the inplace flag because we can safely change the tensors inplace when normalize is used.\n",
    "                # For is_train=False, we can safely change the tensors inplace because we do it only once, when caching.\n",
    "                # For is_train=True, we can safely change the tensors inplace because we clone the cached tensors first.\n",
    "    return v2.Compose(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = get_init_transform(False, IMAGE_SIZE)\n",
    "inputs = np.ones((32, 32, 3), dtype=np.uint8) * 255\n",
    "shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "check_transforms(transform, inputs, shape, 3, -3, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = get_init_transform(True, IMAGE_SIZE)\n",
    "inputs = np.ones((32, 32, 3), dtype=np.uint8) * 255\n",
    "shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "check_transforms(transform, inputs, shape, 255, 0, torch.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transforms(image_size: int):\n",
    "    # These transformations are cached.\n",
    "    # We could have used RandomCrop with padding. But we are smart, and we know we cache the initial_transforms so\n",
    "    # we don't compute them during runtime. Therefore, we do the padding beforehand, and apply cropping only at\n",
    "    # runtime\n",
    "    random_choice = v2.RandomChoice([\n",
    "        v2.RandomPerspective(\n",
    "                    distortion_scale=0.15, # controls how much each corner can move. \n",
    "                    p=1.0),                # probability of applying the effect\n",
    "        v2.RandomRotation(degrees=30),     # rotates an image with random angle\n",
    "        v2.RandomAffine(\n",
    "                    degrees=30,             # rotation ±30\n",
    "                    translate=(0.15, 0.15), # horizontal/vertical translation as fraction of image\n",
    "                    scale=(0.75, 1.05),     # scale factor\n",
    "                    shear=10),              # shear angle ±10°\n",
    "        v2.RandomCrop(\n",
    "                    size=image_size,   # height & width of crop\n",
    "                    padding=4),        # pixels to pad around the image\n",
    "        v2.RandomResizedCrop(\n",
    "                    size=image_size,\n",
    "                    scale=(0.75, 1.),  # range of area proportion to crop from the original image\n",
    "                    ratio=(0.8,  1.)), # range of aspect ratio (width/height)\n",
    "        v2.RandomAdjustSharpness(\n",
    "                    sharpness_factor=1.5, # controls the degree of sharpness; ( >1 sharpened; <1 slightly blurred)\n",
    "                    p=1.),                      # probability of applying the transform\n",
    "        v2.RandomAutocontrast(p=1.), # probability of applying the transform\n",
    "        v2.RandomEqualize( # histogram of pixel values\n",
    "                    p=1.), # probability of applying the transform\n",
    "        v2.ColorJitter(  # randomly changes the brightness, contrast, saturation, and hue\n",
    "                    brightness=0.5, # factor to change brightness\n",
    "                    contrast=0.3,   # factor to change contrast\n",
    "                    saturation=0.3, # factor to change saturation\n",
    "                    hue=0.3,),      # factor to change hue\n",
    "        v2.GaussianBlur(  # applies a Gaussian blur\n",
    "                    kernel_size=(7, 7), # size of the Gaussian kernel\n",
    "                    # standard deviation of the Gaussian kernel; a float or tuple (min, max) for random sampling\n",
    "                    sigma=(0.1, 5.)),   # how to handle image borders\n",
    "        v2.RandomErasing(\n",
    "                    scale=(0.01, 0.15), # range of area ratio to erase (relative to image area)\n",
    "                    value=10,           # fill value: single number, tuple, or 'random'\n",
    "                    inplace=False,      # whether to erase in place or return a new image\n",
    "                    p=1.),              # probability of applying the transform\n",
    "        v2.Grayscale(num_output_channels=3), # number of channels in output image: 1 or 3\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.Identity(),  # returns the input image unchanged\n",
    "    ])\n",
    "    transforms = v2.Compose([random_choice,\n",
    "        v2.ToDtype(torch.float32, scale=True), # converts uint8 [0,255] -> float32 [0,1]\n",
    "        v2.Normalize(mean=(0.491, 0.482, 0.446), \n",
    "                     std=(0.247, 0.243, 0.261), \n",
    "                     inplace=True),\n",
    "        ])\n",
    "    # We use the inplace flag because we can safely change the tensors inplace when normalize is used.\n",
    "    # For is_train=False, we can safely change the tensors inplace because we do it only once, when caching.\n",
    "    # For is_train=True, we can safely change the tensors inplace because we clone the cached tensors first.\n",
    "\n",
    "    # Q: How to make this faster?\n",
    "    # A: Use batched runtime transformations. y\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform = get_transforms(IMAGE_SIZE)\n",
    "inputs = torch.ones((3, IMAGE_SIZE, IMAGE_SIZE), dtype=torch.uint8) * 255\n",
    "shape = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "check_transforms(transform, inputs, shape, 3, -3, torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### All transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_transforms(num_classes: int = 10):\n",
    "    transforms = v2.RandomChoice([\n",
    "                v2.CutMix(num_classes=num_classes),  # See the CutMix paper\n",
    "                v2.MixUp(num_classes=num_classes),   # See the MixUp paper\n",
    "                LabelSmoothing(num_classes=num_classes, smooth_size=0.13),  # A third of all times, don't use neither CutMix nor MixUp.\n",
    "            ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomChoice(transforms=[CutMix(alpha=1.0, num_classes=10), MixUp(alpha=1.0, num_classes=10), LabelSmoothing(num_classes=10, smooth_size=0.13)], p=[0.3333333333333333, 0.3333333333333333, 0.3333333333333333])\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform = get_all_transforms(num_classes=NUM_CLASSES)\n",
    "inputs  = torch.ones((BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE), dtype=torch.float32)\n",
    "outputs = torch.zeros((BATCH_SIZE,), dtype=torch.int64)\n",
    "print(transform)\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = (BATCH_SIZE, NUM_CLASSES)\n",
    "check_all_transforms(transform, inputs, outputs, in_shape, out_shape, 3, -3, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tmp_l = LabelSmoothing(num_classes=10, smooth_size=0.13)\n",
    "y = torch.zeros((BATCH_SIZE,), dtype=torch.int64)\n",
    "for _ in range(1000):\n",
    "    tmp0, tmp1 = tmp_l(None, y)\n",
    "    if (torch.less(tmp1, 0).sum() > 0):\n",
    "        print(torch.less(tmp1, 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Aquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cifar10_train = CIFAR10(root=\"./data\", train=True, transform=None, download=True)\n",
    "print(str(cifar10_train))\n",
    "cifar10_test = CIFAR10(root=\"./data\", train=False, transform=None, download=True)\n",
    "print(str(cifar10_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d_train = dict(inputs=np.array(cifar10_train.data, dtype=np.uint8), \n",
    "               targets=np.array(cifar10_train.targets, dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)\n",
    "d_test  = dict(inputs=np.array(cifar10_test.data,  dtype=np.uint8), \n",
    "               targets=np.array(cifar10_test.targets,  dtype=np.uint16), \n",
    "               num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train[\"inputs\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6uR-23mOg8S",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Efficient in-memory dataset wrapper for caching\n",
    "\n",
    "Beware that this dataset keeps all data in memory. If it is too large, we might opt to cache the data on the disk and read it in `__getitem__()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_ds = DatasetRandAppend(d_train, transform=get_init_transform(True, IMAGE_SIZE), \n",
    "                             train=True,  rand_class_size=20, freq_rand=10)\n",
    "test_ds  = DatasetRandAppend(d_test,  transform=get_init_transform(False, IMAGE_SIZE), \n",
    "                             train=False)\n",
    "eval_ds  = DatasetRandAppend(d_train, transform=get_init_transform(False, IMAGE_SIZE), \n",
    "                             train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_shape  = (3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "#in_shape  = (32, 32, 3)\n",
    "out_shape = ()\n",
    "for idx in range(1000):\n",
    "    inputs, outputs = train_ds[idx]\n",
    "    #print(type(outputs))\n",
    "    check_inputs(inputs,  in_shape,  255, 0, torch.uint8, arr_type=torchvision.tv_tensors._image.Image)\n",
    "    check_inputs(outputs, out_shape,  30, 0, np.int64, arr_type=np.int64)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform     = get_transforms(IMAGE_SIZE)\n",
    "all_transform = get_all_transforms(num_classes=train_ds.num_classes)\n",
    "print(transform)\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = (BATCH_SIZE, train_ds.num_classes)\n",
    "for idx, (inputs, outputs) in zip(range(1000), train_dl):\n",
    "    x    = transform(inputs)\n",
    "    check_inputs(x, in_shape, 3, -3, torch.float32)\n",
    "    x, y = all_transform(x, outputs)\n",
    "    check_inputs(x, in_shape, 3, -3, torch.float32)\n",
    "    check_inputs(y, out_shape, 1, 0, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = (BATCH_SIZE, train_ds.num_classes)\n",
    "for idx, (inputs, outputs) in zip(range(1000), test_dl):\n",
    "    check_inputs(inputs, in_shape, 3, -3, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@timed(use_seconds=True, show_args=True, return_time=True)\n",
    "def load_data(dataset, num_workers: int):\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, drop_last=False)\n",
    "    for _ in dataloader:\n",
    "        pass  # Simulate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(33):\n",
    "    _, t0 = load_data(test_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = []\n",
    "freeze_support()\n",
    "for num_workers in range(33):\n",
    "    _, t0 = load_data(train_ds, num_workers)\n",
    "    times.append(t0)\n",
    "print(\"argmin {}\".format(np.argmin(times)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# select the best number workers\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=8, drop_last=False)\n",
    "test_dl  = DataLoader(test_ds , batch_size=BATCH_SIZE, shuffle=False, num_workers=3, drop_last=False)\n",
    "eval_dl  = DataLoader(eval_ds,  batch_size=BATCH_SIZE, shuffle=True,  num_workers=8, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomChoice(transforms=[CutMix(alpha=1.0, num_classes=30), MixUp(alpha=1.0, num_classes=30), LabelSmoothing(num_classes=30, smooth_size=0.13)], p=[0.3333333333333333, 0.3333333333333333, 0.3333333333333333])\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform     = get_transforms(IMAGE_SIZE)\n",
    "all_transform = get_all_transforms(num_classes=train_ds.num_classes)\n",
    "print(all_transform)\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = (BATCH_SIZE, train_ds.num_classes)\n",
    "for idx, (inputs, outputs) in zip(range(1000), train_dl):\n",
    "    check_inputs(inputs, in_shape, 255, 0, torch.uint8, arr_type=torch.Tensor)\n",
    "    x    = transform(inputs)\n",
    "    check_inputs(x, in_shape, 3, -3, torch.float32)\n",
    "    x, y = all_transform(x, outputs)\n",
    "    check_inputs(x, in_shape, 3, -3, torch.float32)\n",
    "    check_inputs(y, out_shape, 1.13, -0.13, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error: shape should be '(24, 3, 32, 32)', is 'torch.Size([16, 3, 32, 32])'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m out_shape = (BATCH_SIZE, train_ds.num_classes)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, (inputs, outputs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m), test_dl):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mcheck_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOK\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mcheck_inputs\u001b[39m\u001b[34m(inputs, shape, max_val, min_val, dtype, arr_type)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#check shape\u001b[39;00m\n\u001b[32m     16\u001b[39m error_msg = \u001b[33m\"\u001b[39m\u001b[33mError: shape should be \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, is \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m.format(shape, inputs.shape)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (inputs.shape == shape), error_msg\n",
      "\u001b[31mAssertionError\u001b[39m: Error: shape should be '(24, 3, 32, 32)', is 'torch.Size([16, 3, 32, 32])'"
     ]
    }
   ],
   "source": [
    "\n",
    "in_shape  = (BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "out_shape = (BATCH_SIZE, train_ds.num_classes)\n",
    "for idx, (inputs, outputs) in zip(range(1000), test_dl):\n",
    "    check_inputs(inputs, in_shape, 3, -3, torch.float32)\n",
    "else:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okXAgQ-DOg8T"
   },
   "source": [
    "This is the classification model, which leverages PyTorch Image Models to create backbones.\n",
    "\n",
    "Beware that not all backbones have a fully connected (fc) layer at the end. Some of them do, especially the resnet variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input=(img_channels, out_channels, kernel_size, stride)\n",
    "body={body_name={in_channels, expansion, stride, intermediate_channels, num_residual_blocks}}\n",
    "Output=(in_features, out_features)\n",
    "\"\"\"\n",
    "resnet_conf = dict(\n",
    "    Input=dict(\n",
    "        img_channels=3, \n",
    "        out_channels=9, \n",
    "        kernel_size=3, \n",
    "        stride=1),\n",
    "    body=dict(\n",
    "        conv_2x=dict(in_channels=9, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=32, \n",
    "                     num_residual_blocks=3),\n",
    "        conv_3x=dict(in_channels=128, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=64, \n",
    "                     num_residual_blocks=4),\n",
    "        conv_4x=dict(in_channels=256, \n",
    "                     expansion=4, \n",
    "                     stride=2, \n",
    "                     intermediate_channels=256, \n",
    "                     num_residual_blocks=3),\n",
    "    ),\n",
    "    Output=(1024, train_ds.num_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "170*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(\"resnet_32x32_cifar\", **resnet_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (input): InputStride(\n",
       "    (conv1): Conv2d(3, 9, kernel_size=(3, 3), stride=(1, 1), groups=3, bias=False)\n",
       "    (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (body): Sequential(\n",
       "    (conv_2x_0): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(9, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_downsample): IdentityConv2dDownSample(\n",
       "        (conv1): Conv2d(9, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv_2x_1): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_2x_2): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_3x_0): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_downsample): IdentityConv2dDownSample(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv_3x_1): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_3x_2): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_3x_3): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_4x_0): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_downsample): IdentityConv2dDownSample(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv_4x_1): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_4x_2): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_5x_0): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (identity_downsample): IdentityConv2dDownSample(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv_5x_1): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv_5x_2): IdentityResNetModule(\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc): Linear(in_features=1024, out_features=30, bias=True)\n",
       "  (softmax): Softmax(dim=0)\n",
       ")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#all_transforms=None\n",
    "trainer_obj = Trainer(\n",
    "            model,\n",
    "            optimizer,\n",
    "            criterion,\n",
    "            use_cpu=False,\n",
    "            type_compile=\"compile\",\n",
    "            disable_tqdm=True, \n",
    "            transforms=transform, \n",
    "            all_transforms=all_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_epoch 0    100\n",
      "1    100\n",
      "Name: epoch, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCH = 100\n",
    "history_path = \"{}/{}\".format(LCL_PATH, \"logs/deep_5x_conf_logs.csv\")\n",
    "run_conf_obj = RunConfigs(model, trainer_obj, epochs=EPOCH, train_dl=train_dl, val_dl=test_dl, history_path=history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adam': {'optimizer': torch.optim.adam.Adam,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_momentum_nesterov': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_momentum_nesterov_weight_decay': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9,\n",
       "   'nesterov': True,\n",
       "   'weight_decay': 0.01,\n",
       "   'fused': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'adamW': {'optimizer': torch.optim.adamw.AdamW,\n",
       "  'opt_hyperparameters': {'fused': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': None,\n",
       "  'lr_scheduler_hyperparameters': None},\n",
       " 'sgd_scheduler': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}},\n",
       " 'sgd_momentum_nesterov_scheduler': {'optimizer': torch.optim.sgd.SGD,\n",
       "  'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True},\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}},\n",
       " 'adam_scheduler': {'optimizer': torch.optim.adam.Adam,\n",
       "  'opt_hyperparameters': None,\n",
       "  'lr': 0.001,\n",
       "  'lr_scheduler': torch.optim.lr_scheduler.OneCycleLR,\n",
       "  'lr_scheduler_hyperparameters': {'max_lr': 0.1,\n",
       "   'steps_per_epoch': 10,\n",
       "   'epochs': 100}}}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# name:(optimizer, (lr_scheduler=..., opt_hyperparameters=..., lr=float))\n",
    "\n",
    "multiple_runs_conf = dict(\n",
    "    adam=dict(optimizer=torch.optim.Adam,\n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd_momentum_nesterov=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    sgd_momentum_nesterov_weight_decay=dict(optimizer=torch.optim.SGD, \n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True, weight_decay=0.01, fused=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    adamW=dict(optimizer=torch.optim.AdamW,\n",
    "            opt_hyperparameters=dict(fused=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=None,\n",
    "            lr_scheduler_hyperparameters=None,\n",
    "    ),\n",
    "    # scheduler\n",
    "    sgd_scheduler=dict(optimizer=torch.optim.SGD,\n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    "    sgd_momentum_nesterov_scheduler=dict(optimizer=torch.optim.SGD,\n",
    "            opt_hyperparameters=dict(momentum=0.9, nesterov=True),\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    "    adam_scheduler=dict(optimizer=torch.optim.Adam, \n",
    "            opt_hyperparameters=None,\n",
    "            lr=0.001,\n",
    "            lr_scheduler=torch.optim.lr_scheduler.OneCycleLR,\n",
    "            lr_scheduler_hyperparameters=dict(max_lr=0.1, steps_per_epoch=10, epochs=EPOCH), \n",
    "    ),\n",
    ")\n",
    "multiple_runs_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'adam', conf {'optimizer': <class 'torch.optim.adam.Adam'>, 'opt_hyperparameters': None, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [1:08:45<00:00, 41.26s/it, train_acc=0.688, train_loss=0.163, val_acc=0.866, val_loss=0.0372]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'sgd', conf {'optimizer': <class 'torch.optim.sgd.SGD'>, 'opt_hyperparameters': None, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                                                                                                                                                 | 0/100 [00:00<?, ?it/s]W1216 18:45:18.574000 5757 torch/_dynamo/convert_frame.py:1358] [3/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1216 18:45:18.574000 5757 torch/_dynamo/convert_frame.py:1358] [3/8]    function: 'wrapper' (/home/gheorghe/venvs/torch_env/lib/python3.12/site-packages/torch/optim/optimizer.py:497)\n",
      "W1216 18:45:18.574000 5757 torch/_dynamo/convert_frame.py:1358] [3/8]    last reason: 3/0: Cache line invalidated because L['args'][0] got deallocated\n",
      "W1216 18:45:18.574000 5757 torch/_dynamo/convert_frame.py:1358] [3/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1216 18:45:18.574000 5757 torch/_dynamo/convert_frame.py:1358] [3/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [1:03:09<00:00, 37.90s/it, train_acc=0.633, train_loss=0.169, val_acc=0.793, val_loss=0.0503]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'sgd_momentum_nesterov', conf {'optimizer': <class 'torch.optim.sgd.SGD'>, 'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True}, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [1:11:46<00:00, 43.06s/it, train_acc=0.724, train_loss=0.164, val_acc=0.855, val_loss=0.0385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start running name: 'sgd_momentum_nesterov_weight_decay', conf {'optimizer': <class 'torch.optim.sgd.SGD'>, 'opt_hyperparameters': {'momentum': 0.9, 'nesterov': True, 'weight_decay': 0.01, 'fused': True}, 'lr': 0.001, 'lr_scheduler': None, 'lr_scheduler_hyperparameters': None}\n",
      "Running 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|█████████████████████████████████████████████████████████████████▏                                                                   | 49/100 [34:08<35:31, 41.80s/it, train_acc=0.555, train_loss=0.188, val_acc=0.754, val_loss=0.0537]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrun_conf_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmultiple_runs_conf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/conf_manager/train_conf.py:56\u001b[39m, in \u001b[36mRunConfigs.__call__\u001b[39m\u001b[34m(self, **confs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     55\u001b[39m     conf = {}\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_conf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/conf_manager/train_conf.py:45\u001b[39m, in \u001b[36mRunConfigs.run\u001b[39m\u001b[34m(self, name, **conf)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# antreneaza modelul\u001b[39;00m\n\u001b[32m     44\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m logs[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] = name\n\u001b[32m     47\u001b[39m logs[\u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m] = time.time()-start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/trainer/trainer.py:135\u001b[39m, in \u001b[36mTrainer.run\u001b[39m\u001b[34m(self, train_dl, val_dl, epochs, save_path)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc=\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m         tr_loss, tr_acc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m         va_loss, va_acc = \u001b[38;5;28mself\u001b[39m.val(val_dl)\n\u001b[32m    137\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m va_acc > \u001b[38;5;28mself\u001b[39m.best_va_acc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/trainer/trainer.py:80\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, train_ds)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.all_transforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     78\u001b[39m     data, target = \u001b[38;5;28mself\u001b[39m.all_transforms(data, target)\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m predicted, loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# We do this when cutmix or mixup was used, transforming the hard labels into soft labels\u001b[39;00m\n\u001b[32m     84\u001b[39m     target = target.argmax(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:832\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    829\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Proiecte/master/CapitoleAvansateDinReteleNeuronale/deep_learning/trainer/trainer.py:54\u001b[39m, in \u001b[36mTrainer.step\u001b[39m\u001b[34m(self, data, target)\u001b[39m\n\u001b[32m     52\u001b[39m predicted = \u001b[38;5;28mself\u001b[39m.model(data)\n\u001b[32m     53\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(predicted, target)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.step()\n\u001b[32m     56\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_env/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_env/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/torch_env/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_conf_obj(**multiple_runs_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOhQ2VIMOg8V"
   },
   "source": [
    "The comments are self-explainatory. If you do not know what a transformation does, the official documentation is your friend.\n",
    "Reading documentation helps your brain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q47ymPCBOg8Z"
   },
   "source": [
    "The full training script is available in [complex_yet_simple_training_pipeline.py](./complex_yet_simple_training_pipeline.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r36R_vT5Og8Z"
   },
   "source": [
    "## Excercises\n",
    "\n",
    "1. Create your own efficient training pipeline for CIFAR-10.\n",
    "2. Adapt your pipeline (and this pipeline) to use some batched transformations. Measure the speedup!\n",
    "3. Adapt your pipeline (and this pipeline) to include Automatic Mixed Precision. Read the documentation first!\n",
    "4. Adjust your pipeline (or this pipeline) to achieve 96% on CIFAR-10 (hard). You may change the model, but pretrained weights are forbidden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F31jwuDOg8Z"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVwcPWNsOg8a"
   },
   "source": [
    "| All     | [advanced_pytorch/](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch) |\n",
    "|---------|-- |\n",
    "| Current | [A complex yet simple efficient training pipeline for CIFAR-10](https://github.com/Tensor-Reloaded/AI-Learning-Hub/blob/main/resources/advanced_pytorch/ComplexYetSimpleTrainingPipeline.ipynb) |"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
